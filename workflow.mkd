# Log into uppmax
ssh [username]@rackham.uppmax.uu.se

# Pull the data

## Create softlinks of bam files to personal working directory
```bash
# Move to working/parent directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

mkdir Data # make a new directory within the parent directory
cd Data # move into Data

# Create a softink to the bamfiles within the Data directory
ln -s /proj/sparrowhybridization/Pitaliae/data/WGS/bamfiles/ Bamfiles
```

## Download the new house sparrow reference genome from NCBI (accession: GCA_036417665.1)
```bash
# In the Data directory
mkdir ReferenceGenome
cd ReferenceGenome

# Command line download unsuccessful, download to local computer then move to uppmax server: https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_036417665.1/
scp ncbi_dataset.zip [username]@rackham.uppmax.uu.se:/proj/sparrowhybridization/Pitaliae/working/Arianna/Data/ReferenceGenome

# Investigate headers
cat GCA_036417665.1_bPasDom1.hap1_genomic.fna | grep \>
```

# Get basic alignment stats for existing BAM files using SAMtools flagstat (v1.12)
```bash
# Move back to personal working directory
cd ../../

# Create a new directory
mkdir 01_FlagstatExistingBAM
cd 01_FlagstatExistingBAM

# Start an interactive environment
interactive -A naiss2023-5-262 -n 16 -t 1:00:00 # should take ~45 minutes


interactive -A naiss2023-5-262 -n 1 -t 3:00:00


module load bioinfo-tools
module load samtools/1.14

# Run samtools on each bam file
for dir in ../Data/Bamfiles/*/; do # ${dir} contains entire path

    dirName=$(basename "$dir") # just the name of the specific directory is stored (e.g corsica)

    outputDir="${dirName}_flagstats" # store a directory name (e.g. corsica_flagstats)

    mkdir "$outputDir" # make a directory of this name in the current directory, 01_FlagstatsExistingBAM

    for bam in ${dir}/*.bam; do # for each file in the directory that ends in .bam

        file=$(basename "$bam" .bam); # pull the basename, e.g. "K032_resorted_nodup_realigned"

        if [[ "$file" == Rimini* || "$file" == Lesina* ]]; # Rimini and Lesina populations follow a different format
        then
            prefix=${file} # pull the basename, e.g. "Rimini_112"
        else
            prefix=$(basename "$bam" .bam | cut -d "_" -f 1) # for all other populations, pull the basename "K032_resorted_nodup_realigned" then just "K032"
        fi

        samtools flagstat "${bam}" -@ 16 > "${outputDir}/${prefix}.flagstat" # run w/16 threads on each .bam file, save (e.g. "K032.flagstat") to corresponding directory

    done
done
```

Outputs: 1 output per .bam file for each population. Outputs are named [id].flagstat and housed within a subdirectory of 01_FlagstatExistingBAM named [population]_flagstats

# Re-map BAM files to new House Sparrow Reference Genome using SAMtools (v1.12)

## Create a directory for remapping task
```bash
# Move to working/parent directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

mkdir 02_Remap # create a new subdirectory
```

## Index the new reference genome with BWA (v0.7.17)
```bash
# When attempting to run Heng Li's code on remapping an aligned BAM, received this error: [E::bwa_idx_load_from_disk] fail to locate the index files
# To resolve, index the new reference genome

interactive -A naiss2023-5-262 -n 12 -t 0:45:00 # request an interactive environment

module load bioinfo-tools
module load samtools/1.12 # latest version at the time of Heng Li's post
module load bwa/0.7.17 #latest version at the time of Heng Li's post

# Move to directory that houses the new reference genome
cd Data/ReferenceGenome

# Prior to remapping, run bwa index
bwa index GCA_036417665.1_bPasDom1.hap1_genomic.fna # should take ~ 45 minutes
```

Ouputs: 5 files with a basename of GCA_036417665.1_bPasDom1.hap1_genomic.fna (the file name of the reference genome)

(1) [basename].amb (2) [basename].ann (3)[basename].bwt (4) [basename].pac (5) [basename].sa

## Create a folder to house project scripts & create scripts
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

mkdir Scripts
cd Scripts

mkdir 02_Remap_scripts
cd 02_Remap_scripts # scripts for this task should be located here!
```

There should be 12 scripts in this folder! Each script will remap the bam files for a population (population specified in script name). Originally wrote one script to remap all the bam files for every population,but it would take many days to run this script on one node. Therefore, we will run 1 job per population instead of 1 job for all the bam files!

### Example code for remapping just one BAM file
```bash
interactive -A naiss2023-5-262 -n 20 -t 2:00:00

module load bioinfo-tools
module load samtools/1.12 # latest version at the time of Heng Li's post
module load bwa/0.7.17 # latest version at the time of Heng Li's post

# Code pulled from Heng Li's blogpost, see link for command-by-command description
# https://lh3.github.io/2021/07/06/remapping-an-aligned-bam
# 20 cores per node, 1 thread per core! Specify 20 threads for each task
samtools collate -Oun128 ../Data/Bamfiles/corsica/K006_resorted_nodup_realigned.bam -@ 20 | samtools fastq -OT RG,BC -@ 20 - \
| bwa mem -pt8 -CH <(samtools view -H ../Data/Bamfiles/corsica/K006_resorted_nodup_realigned.bam|grep ^@RG) ../Data/ReferenceGenome/GCA_036417665.1_bPasDom1.hap1_genomic.fna -t 20 - \
| samtools sort -@ 20 -m4g -o K006_remapped.bam -
```

## Create jobs for each remapping script
```bash
# Corsica, batch job 46179245
sbatch -A naiss2023-5-262 -p node -t 17:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_corsica.err -J remap_existingBAM_corsica --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_corsica.sh

# Crete, batch job 46179242
sbatch -A naiss2023-5-262 -p node -t 7:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_crete.err -J remap_existingBAM_crete --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_crete.sh

# Crotone, batch job 46179247
sbatch -A naiss2023-5-262 -p node -t 10:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_crotone.err -J remap_existingBAM_crotone --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_crotone.sh

# Guglionesi, batch job 46179248
sbatch -A naiss2023-5-262 -p node -t 12:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_guglionesi.err -J remap_existingBAM_guglionesi --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_guglionesi.sh

# House, batch job 46220281
sbatch -A naiss2023-5-262 -p node -t 15:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_house.err -J remap_existingBAM_house --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_house.sh

# Malta, batch job 46179250
sbatch -A naiss2023-5-262 -p node -t 13:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_malta.err -J remap_existingBAM_malta --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_malta.sh

# Miscellaneous, batch job 46220282
sbatch -A naiss2023-5-262 -p node -t 13:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_miscellaneous.err -J remap_existingBAM_miscellaneous --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_miscellaneous.sh

# P_montanus, batch job 46220283
sbatch -A naiss2023-5-262 -p node -t 13:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_p_montanus.err -J remap_existingBAM_p_montanus --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_p_montanus.sh

# Rimini, batch job 46179255
sbatch -A naiss2023-5-262 -p node -t 9:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_rimini.err -J remap_existingBAM_rimini --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_rimini.sh

# Sardinia, batch job 46179256
sbatch -A naiss2023-5-262 -p node -t 10:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_sardinia.err -J remap_existingBAM_sardinia --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_sardinia.sh

# Sicily, batch job 46179347
sbatch -A naiss2023-5-262 -p node -t 9:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_sicily.err -J remap_existingBAM_sicily --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_sicily.sh

# Spanish, batch job 46179498
sbatch -A naiss2023-5-262 -p node -t 14:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/02_Remap -e ../Job_logs/02_Remap_logs/remapping_spanish.err -J remap_existingBAM_spanish --mail-user=ar4666al-s@student.lu.se --mail-type=ALL remap_spanish.sh
```

## Index the new BAM files using SAMtools (v1.12)
```bash
# Create a script (index_remapped.sh) to index the bam files and save it to Arianna/Scripts/02_Remap_scripts

# Move into the Arianna/Scripts/02_Remap_scripts directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/02_Remap_scripts

# Indexing, batch job 46239392
sbatch -A naiss2023-5-262 -p core -n 16 -t 1:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna -e Job_logs/02_Remap_logs/indexing.err -J indexing_remapped_BAM --mail-user=ar4666al-s@student.lu.se --mail-type=ALL index_remapped.sh
```
Outputs: 1 output per bam file. Outputs are named [id]_remapped.bam.bai and housed in the same directory as the corresponding bam files

# Run SAMtools (v1.14) flagstat on the new BAM files
```bash
# Create a new directory to store flagstat output of task
cd ../.. # back to Arianna working directory
mkdir 03_FlagstatRemappedBAM

# Move to Scripts directory
cd Scripts

# Create a scripts directtory for the task, create flagstat_remapped.sh script and store it here
mkdir 03_FlagstatRemappedBAM_scripts
cd 03_FlagstatRemappedBAM_scripts

# Flagstats on remapped bam files, batch job 46240169
sbatch -A naiss2023-5-262 -p core -n 16 -t 1:30:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/03_FlagstatRemappedBAM -e ../Job_logs/03_FlagstatRemappedBAM_logs/flagstat.err -J flagstat_remapped_BAM --mail-user=ar4666al-s@student.lu.se --mail-type=ALL flagstat_remapped.sh
```
Outputs: 1 output per .bam file for each population. Outputs are named [id]_remapped.flagstat and housed within a subdirectory of 03_FlagstatRemappedBAM named [population]_remap_flagstats

# Merge samtools flagstat output for each existing and remapped bam
``` bash
# Create a new directory to store merged flagstat files
cd ../.. # back to Arianna working directory
mkdir 04_FlagstatCombined
cd 04_FlagstatCombined

# Combine the flagstat files
for dir_existing in ../01_FlagstatExistingBAM/*/; do # ${dir_existing} contains entire path

    dir_existingName=$(basename "$dir_existing") # just the name of the specific directory is stored (e.g corsica_flagstats)

    outputDir="${dir_existingName}_combined" # store a directory name (e.g. corsica_flagstats_combined)

    mkdir "$outputDir" # make a directory of this name in the current directory, 04_FlagstatCombined

    for dir_remapped in ../03_FlagstatRemappedBAM/*/; do

        for flagstat_existing in ${dir_existing}/*.flagstat; do # for the existing bam flagstat files

            prefix_existing=$(basename "$flagstat_existing" .flagstat) # pull just K032 instead of K032.flagstat
            
            for flagstat_remapped in ${dir_remapped}/*.flagstat; do # for the remapped bam flagstat files

                prefix_remapped=$(basename "$flagstat_remapped" "_remapped.flagstat") # pull just Rimini_91 from Rimini_91_remapped.flagstat


                if [[ "$prefix_existing" == "$prefix_remapped" ]]; # if the prefixes for existing and remapped match (e.g. Rimini_91 == Rimini_91)
                then
                    echo -e ">\n>${prefix_existing}.bam flagstat output (existing bam file)" > tempHeader_exisiting # create a temporary header for the existing flagstat output, overwrite for each iteration

                    echo -e ">\n>${prefix_remapped}_remapped.bam flagstat output (remapped bam file)" > tempHeader_remapped # create a temporary header for the remapped flagstat output, overwrite for each iteration

                    cat tempHeader_exisiting ${flagstat_existing} tempHeader_remapped ${flagstat_remapped} > "${outputDir}/${prefix_existing}_combo.flagstat" # vertically concatenate the existing header + existing flagstat output + remapped header + remapped flagstat output
                else
                    continue # if the prefixes don't match, don't do anything
                fi

            done

        done
        
    done
done
```
Outputs: 1 output per sample file for each population. Outputs are named [id]_combo.flagstat and housed within a subdirectory of 04_FlagstatCombined named [population]_flagstats_combined

# Variant Calling

## Add prefix to Malta miscellaneous samples
```bash
# There are different files with the same sample name in the malta_remap folder and miscellaneous_remap
# bcftools will only include one file if another is a duplicate (regardless of changing file names, scans contents of file to determine)

# add "misc" prefix to the "M" samples in the miscellaneous folder
# to differentiate between the two duplicate sample IDs and check vcf header to be sure the samples from malta_remap were used to call variants

cd ../02_Remap/miscellaneous_remap/ # move to directory containing remapped miscellaneous bam files

for bam in M*; do # for files in this directory that start with "M"

    # Append "misc" to the start of the file name
    newName="misc_$bam" 

    # Rename the file
    mv "$bam" "$newName" # e.g. misc_M019_remapped.bam
done

# Since we know that 2 samples won't be inlcuded, we know that 124 samples will actually be included in the VCF! Still call the files 126 sample vcf for now
```

## Generate all site BCFs using BCFtools (v 1.14)
```bash
# Move to home working directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

# Create a subdirectory for the task
mkdir 05_AllSiteBCFs
cd 05_AllSiteBCFs

# Create a file which lists the path to every bam file on a new line
for dir in ../02_Remap/*_remap/; do # ${dir} contains entire path

    for bam in ${dir}/*.bam; do # stores the path to each file that ends in .bam to ${bam}

        echo ${bam} >> mappedReads_paths # Will create the file and append the path stored in ${bam} for each iteration

    done
done

# Create a file with the names of all the chromosomes present in the reference, .fai is the index file!
cat ../Data/ReferenceGenome/GCA_036417665.1_bPasDom1.hap1_genomic.fna.fai | cut -f 1 >> chromo_list # "chromosomes" based on fna header so e.g. CM071426.1 = chromosome 1

# Move to the Scripts directory
cd ../Scripts/

# Create a subdirectory
mkdir 05_AllSiteBCFs_scripts
cd 05_AllSiteBCFs_scipts

# Modify Kalle's script called bcftools.sh (& turn it into an array job) and move it into this directory. Now called bcftools_array.sh

# Submit the array job to call variants for each chromosome & scaffold separateley, batch job 46316581
sbatch -A naiss2023-5-262 --array=1-334 --ntasks=1 -t 4-00:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/05_AllSiteBCFs -e ../Job_logs/05_AllSiteBCFs_logs/126samples_bcf.err -J 126samples_bcf --mail-user=ar4666al-s@student.lu.se --mail-type=ALL bcftools_array.sh
```
Outputs: 1 output BCF file per region (chromosome/scaffold), 334 BCF files total. Outputs are named 126sample_v_gc_raw_[regionID].bcf and are located in the 05_AllSiteBCFs directory

## Generate BCFs with just biallelic sites using BCFtools (v 1.14)
```bash
# Move to home working directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

# Create a subdirectory for the task
mkdir 06_BiallelicSiteBCFs
cd 06_BiallelicSiteBCFs

# Create a file with the paths to each all-site bcf file
for bcf in ../05_AllSiteBCFs/*.bcf; do # ${bcf} contains entire path

    echo ${bcf} >> bcf_paths # Will create the file and append the path stored in ${bcf} for each iteration

done

# Move to the Scripts directory
cd ../Scripts

# Create a subdirectory
mkdir 06_BiallelicSiteBCFs_scripts
cd 06_BiallelicSiteBCFs_scripts

# Make a script called bcftools_biallelic.sh to just pull the biallelic site for each of of the bcf files

# Submit job to pull just biallelic variants for each bcf, batch job 46440834
sbatch -A naiss2023-5-262 --array=1-334 --ntasks=20 -t 2:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/06_BiallelicSiteBCFs -e ../Job_logs/06_BiallelicSiteBCFs_logs/biallelic_bcfs.err -J biallelic_bcfs --mail-user=ar4666al-s@student.lu.se --mail-type=ALL bcftools_biallelic.sh
```
Outputs: 1 output BCF file per region (chromosome/scaffold), 334 BCF files total. Outputs are named 126sample_[regionID]_biallelicSites.bcf and are located in the 06_BiallelicSiteBCFs directory

# Determine filtering thresholds

## Randomly sample ~100,000 calls (of biallelic sites) to determine filtering thresholds, following workflow from https://speciationgenomics.github.io/filtering_vcfs/
```bash
# Move back to home working directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

# Create a subdirectory called
mkdir 07_PreFiltering
cd 07_PreFiltering

# Create a file with the paths to each bcf (biallelic sites only)
for bcf in ../06_BiallelicSiteBCFs/*.bcf; do # ${bcf} contains entire path

    echo ${bcf} >> biallelic_bcf_paths # Will create the file and append the path stored in ${bcf} for each iteration

done

# Move to the Scripts directory
cd ../Scripts

# Create a subdirectory
mkdir 07_PreFiltering_scripts
cd 07_PreFiltering_scripts

# Make a script called countCallsBCF.sh to count the number of calls in each bcf and append that number to a list called countCallsList

# Submit array job to count calls for each bcf (biallelic calls only), batch job 46441607
sbatch -A naiss2023-5-262 --array=1-334 --ntasks=1 -t 1:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/countCalls.err -J countCalls --mail-user=ar4666al-s@student.lu.se --mail-type=ALL countCallsBCF.sh

# Use awk to count all the numbers in the file
cd ../../07_PreFiltering/
awk '{sum+=$1} END {print sum}' countCallsList # 66,887,744 calls -> multuply by 0.00145 = ~ 100,000 calls
```

```bash
# Randomly pull sites from each BCF (only biallelic sites) to equal ~100,000 sites and then concatenate, will take approx 2 hours. Alternatively, could have randomly pull ~100,000 calls from a concatenated bcf (biallelic sites only), but would take 20+ hours

# Move to scripts directory
cd ../Scripts/07_PreFiltering_scripts

# Make a script called pullSubsets.sh to randomly pull sites from each BCF (biallelic) totalling to ~100,000 calls
# Submit array job, batch job 46443715
sbatch -A naiss2023-5-262 --array=1-334 --ntasks=1 -t 5:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/pullSubsets.err -J pullSubsets --mail-user=ar4666al-s@student.lu.se --mail-type=ALL pullSubsets.sh
```
Outputs, option 1: 1 output VCF file per region (chromosome/scaffold), 334 BCF files total. Outputs are named 126sample_[regionID]_biallelicSites_subset.vcf and are located in the 07_PreFiltering directory

```bash
# Move to scripts directory if not already there
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/07_PreFiltering_scripts

# Create a script called concatSubsetVCFs.sh to concatenate the subset VCFs into one VCF
# Submit job, batch job 46444847
sbatch -A naiss2023-5-262 -p node -t 1:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/concat_subsets.err -J concat_subsets --mail-user=ar4666al-s@student.lu.se --mail-type=ALL concatSubsetVCFs.sh
```
Output: 1 output, compressed VCF file called sparrow_biallelics_subset.vcf.gz located in the 07_PreFiltering directory

```bash
# To make the 07_PreFiltering directory easier to navigate, move the individidual subset files to their own directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering
mkdir Subsets
mv *biallelicSites_subset.vcf ./Subsets
```

```bash
# Move to 07_PreFiltering directory if not already there
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering

# Start an interactive environment
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load bcftools/1.14

# Concatenate autosomes and scaffold subsets into one vcf
for vcf in ./Subsets/*biallelicSites_subset.vcf; do # ${vcf} contains entire path

    prefix=$(basename "$vcf" .vcf)

    if [[ "$prefix" == *CM071456.1* || "$prefix" == *CM071464.1* ]]; # if Z chrom (CM071456.1) or W chrom (CM071464.1)
    then
        continue
    else
        echo ${vcf} >> VCFsubset_paths_autosomal
    fi
done

bcftools concat --threads 20 --file-list VCFsubset_paths_autosomal -Oz -o sparrow_biallelics_subset_AUTOSOMES.vcf.gz

# Concatenate the sex chromosome subsets
for vcf in ./Subsets/*biallelicSites_subset.vcf; do # ${vcf} contains entire path

    prefix=$(basename "$vcf" .vcf)

    if [[ "$prefix" == *CM071456.1* || "$prefix" == *CM071464.1* ]]; # if Z chrom (CM071456.1) or W chrom (CM071464.1)
    then
        echo ${vcf} >> VCFsubset_paths_sex
    else
        continue
    fi
done

bcftools concat --threads 20 --file-list VCFsubset_paths_sex -Oz -o sparrow_biallelics_subset_SEX.vcf.gz

# Create compressed VCFs for the individual sex chromosome subsets
bcftools view ./Subsets/126sample_CM071456.1_biallelicSites_subset.vcf -Oz -o sparrow_biallelics_subset_zChrom.vcf.gz
bcftools view ./Subsets/126sample_CM071464.1_biallelicSites_subset.vcf -Oz -o sparrow_biallelics_subset_wChrom.vcf.gz
```
Output: 4 compressed VCF files, named as shown above, located in the 07_PreFiltering directory

```bash
# index the compressed VCFs
bcftools index sparrow_biallelics_subset.vcf.gz
bcftools index sparrow_biallelics_subset_AUTOSOMES.vcf.gz
bcftools index sparrow_biallelics_subset_SEX.vcf.gz
bcftools index sparrow_biallelics_subset_zChrom.vcf.gz
bcftools index sparrow_biallelics_subset_wChrom.vcf.gz
```
Output: 5 output files that follow this format: [inputFileName].csi, located in the 07_PreFiltering directory 

## Generate stats for the concatenated subset VCFs, following workflow from https://speciationgenomics.github.io/filtering_vcfs/
```bash
# Start new interactive environment if necessary
interactive -A naiss2023-5-262 -n 1 -t 1:00:00
module load bioinfo-tools
module load vcftools/0.1.16
```

### Stats for sparrow_biallelics_subset.vcf.gz
```bash
# Create a subdirectory called VCFtools
mkdir VCFtools

# Set environmental variables
SUBSET_VCF=/proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering/sparrow_biallelics_subset.vcf.gz
OUT=/proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering/VCFtools/sparrow_biallelics_subset

# Calculate allele frequency for each variant
# don't need to specify --max-alleles 2 as we've already filtered to just have biallelic
vcftools --gzvcf $SUBSET_VCF --freq2 --out $OUT  # sparrow_biallelics_subset.frq

# Calculate the mean depth of coverage per individual
vcftools --gzvcf $SUBSET_VCF --depth --out $OUT # sparrow_biallelics_subset.idepth

# Estimate the mean depth of coverage for each site
vcftools --gzvcf $SUBSET_VCF --site-mean-depth --out $OUT # sparrow_biallelics_subset.ldepth.mean

# Extract the site quality score for each site
vcftools --gzvcf $SUBSET_VCF --site-quality --out $OUT # sparrow_biallelics_subset.lqual

# Calculate proportion of missing data per sample/individual
vcftools --gzvcf $SUBSET_VCF --missing-indv --out $OUT # sparrow_biallelics_subset.imiss

# Calculate proportion of missing data per site
vcftools --gzvcf $SUBSET_VCF --missing-site --out $OUT # sparrow_biallelics_subset.lmiss

# Calculate heterozygosity and inbreeding coefficient per individual! Note: individuals are from different pops -> "the expected heterozygosity will be overestimated due to the Wahlund-effect" (source: https://speciationgenomics.github.io/filtering_vcfs/)
vcftools --gzvcf $SUBSET_VCF --het --out $OUT # sparrow_biallelics_subset.het

### Repeat steps from setting environmental variables onward for: sparrow_biallelics_subset_AUTOSOMES.vcf.gz, sparrow_biallelics_subset_SEX.vcf.gz, sparrow_biallelics_subset_zChrom.vcf.gz, sparrow_biallelics_subset_wChrom.vcf.gz
```
Outputs: 35 output files named [inputFilePrefix].[calculationType] located in the VCFtools subdirectory

## Move to R on local computer to visual stat we generated, following workflow from https://speciationgenomics.github.io/filtering_vcfs/

```bash
# Within project directory of local computer, create a directory to store the stats we just generate
mkdir VCF_filteringThreshold_stats
cd VCF_filteringThreshold_stats

# Copy VCFtools directory from server to local
scp -r arianna2@rackham.uppmax.uu.se:/proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering/VCFtools .

# Create an R script within the VCF_filteringThreshold_stats directory called vcfStats.R
```

### Pull subsets again for sex chromosomes, try for a more representative sample to double check, following workflow from https://speciationgenomics.github.io/filtering_vcfs/
```bash
# Move to scripts directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/07_PreFiltering_scripts

# Create jobs to re-run subsetting for Z and W, mainly to get a larger subset for W
# 570,763 biallelic sites for W, pull ~ 80,000 for subset
# 3,398,265 biallelic sites for Z, pull ~ 80,000 for subset

# batch job 46476787
sbatch -A naiss2023-5-262 -p core -n 1 -t 5:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/subsetRedoW.err -J subsetRedoW --mail-user=ar4666al-s@student.lu.se --mail-type=ALL pullSubsets_redoW.sh

# batch job 46476823
sbatch -A naiss2023-5-262 -p core -n 1 -t 5:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/subsetRedoZ.err -J subsetRedoZ --mail-user=ar4666al-s@student.lu.se --mail-type=ALL pullSubsets_redoZ.sh

# Start an interactive session
interactive -A naiss2023-5-262 -n 1 -t 1:00:00
module load bioinfo-tools
module load vcftools/0.1.16

# Compress the new vcf subsets
bcftools view ./Subsets/126sample_CM071456.1_biallelicSites_subset_REDO.vcf -Oz -o sparrow_biallelics_subset_zChrom_REDO.vcf.gz
bcftools view ./Subsets/126sample_CM071464.1_biallelicSites_subset_REDO.vcf -Oz -o sparrow_biallelics_subset_wChrom_REDO.vcf.gz

# index the compressed VCFs
bcftools index sparrow_biallelics_subset_zChrom_REDO.vcf.gz
bcftools index sparrow_biallelics_subset_wChrom_REDO.vcf.gz

## Rerun stats on these new subsets. Repeat steps from setting environmental variables onward for: sparrow_biallelics_subset_zChrom_REDO.vcf.gz, sparrow_biallelics_subset_wChrom_REDO.vcf.gz
```

## Check for duplicate positions before filtering
```bash
# Move to 07_PreFiltering_scripts
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/07_PreFiltering_scripts

# Create a script which checks for duplicate positions called checkDups.sh

# Check for duplicates, batch job 46476877
sbatch -A naiss2023-5-262 --array=1-334 --ntasks=1 -t 2:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering -e ../Job_logs/07_PreFiltering_logs/checkDups.err -J checkdups --mail-user=ar4666al-s@student.lu.se --mail-type=ALL checkDups.sh

# Move to 07_PreFiltering
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/07_PreFiltering

# Open the output file which shows if duplicates exist
cat bcf_contains_dups_list # no duplicates!
```

# Filter
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 08_Filtering
cd 08_Filtering

# Pull paths to biallelic bcfs - AUTOSOMES & Z only, won't look at W for now (very minimal female samples, very low depth)!!
for bcf in ../06_BiallelicSiteBCFs/*bcf; do

    prefix=$(basename "$bcf" .bcf)

    if [[ "$prefix" == *CM071464.1* ]]; # if W chrom (CM071464.1)
    then
        continue # don't do anything
    else
        echo ${bcf} >> biallelic_bcf_paths_autosomesZ # append path to autosomal & Z bcf to the list
    fi
done

# Create two different sub-directories to store vcfs with different filtering parameters
mkdir InclMaxMissing
mkdir SansMaxMissing

# Move to the scripts directory
cd Scripts
mkdir 08_Filtering_scripts

# Create two scripts for filtering! One which contains a filter for the --max-missingess parameter (filterBiallelic_autosomesZ_inclMaxMiss.sh) and one that doesn't (filterBiallelic_autosomesZ_sansMaxMiss.sh)

# Includes max-missingness parameter, submitted batch job 46594888
sbatch -A naiss2023-5-262 --array=1-333 --ntasks=20 -t 6:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/08_Filtering/InclMaxMissing -e ../../Job_logs/08_Filtering_logs/InclMaxMissing/filtering_autosomesZ_%a.err -o ../../Job_logs/08_Filtering_logs/InclMaxMissing/filtering_autosomesZ_%a.out -J filtering_autosomesZ_inclMaxMiss --mail-user=ar4666al-s@student.lu.se --mail-type=ALL filterBiallelic_autosomesZ_inclMaxMiss.sh

# DOES NOT INCLUDE max-missingness parameter, submitted batch job 46611874
sbatch -A naiss2023-5-262 --array=1-333 --ntasks=20 -t 5:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/08_Filtering/SansMaxMissing -e ../../Job_logs/08_Filtering_logs/SansMaxMissing/filtering_autosomesZ_%a.err -o ../../Job_logs/08_Filtering_logs/SansMaxMissing/filtering_autosomesZ_%a.out -J filtering_autosomesZ_sansMaxMiss --mail-user=ar4666al-s@student.lu.se --mail-type=ALL filterBiallelic_autosomesZ_sansMaxMiss.sh
```
Outputs: 333 (not including W chrom!) compressed VCF files in each subdirectory (InclMaxMissing and SansMaxMissing) of 08_Filtering. Files are named 126sample_[regionID]_biallelicSites_filtered.vcf.gz

## Concatenate the scaffold VCFs
```bash
# Request an interactive environment
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load bcftools/1.14

# Move to filtering directory if not there already
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/08_Filtering

# First, work on the filtered VCFs in the InclMaxMissing directory
cd InclMaxMissing

# Make a subdirectory to store filtered scaffolds
mkdir AllScaffolds_biallelicSites

# Move all the scaffolds into that subdriectory
for vcf_gz in ./*_biallelicSites_filtered.vcf.gz; do # ${vcf} contains entire path

    prefix=$(basename "$vcf_gz" .vcf.gz)

    if [[ "$prefix" == *JAZGKA010000010.1* || "$prefix" == *JAZGKA010000030.1* || "$prefix" == *CM0714* ]]; # chromosomes (CM0714), SUPER_8 (JAZGKA010000010.1), SUPER_26 (JAZGKA010000030.1)
    then
        continue
    else
       mv ${vcf_gz} AllScaffolds_biallelicSites
    fi
done

# Concatenate the scaffolds, the output will be located in the InclMaxMissing directory!
bcftools concat --threads 20 AllScaffolds_biallelicSites/*vcf.gz -Oz -o 126sample_scaffolds_biallelicSites_filtered.vcf.gz

# Archive and compress the entire directory containing filtered scaffolds
tar -cvzf AllScaffolds_biallelicSites.tar.gz AllScaffolds_biallelicSites
rm -r AllScaffolds_biallelicSites # remove the non-archived/non-compressed version of the directory

# Follow the same steps as above for the files in the SansMaxMissing directory
```
Outputs: 1 compressed VCF file in each subdirectory (InclMaxMissing and SansMaxMissing) of 08_Filtering called 126sample_scaffolds_biallelicSites_filtered.vcf.gz. 1 archived and compressed directory in each subdirectory (InclMaxMissing and SansMaxMissing) of 08_Filtering called AllScaffolds_biallelicSites.tar.gz

## Index the filtered VCFs
```bash
# Move to 08_Filtering_scripts directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/08_Filtering_scripts

# Create a script to index all the filtered VCF files, call it index_filtered_VCFs.sh

# Submit job to index the filtered VCF files, batch job 46616765
sbatch -A naiss2023-5-262 -p core -n 1 -t 3:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/08_Filtering/ -e ../Job_logs/08_Filtering_logs/indexing.err -J indexing_filtered_VCFs --mail-user=ar4666al-s@student.lu.se --mail-type=ALL index_filtered_VCFs.sh
```

# Designate Alleles
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 09_DesignateAlleles
cd 09_DesignateAlleles
mkdir Inputs
cd Inputs
```
## Prep steps

### Expand and modify the existing sparrows.py python script
Notable changes:
1. Output will include desingation for each SITE in addition to allele designations it already outputs for italians
2. Will output two files, one that only shows "A" for ancestral and another that also shows ancestral parentage "SA" or "HA" for ease of use in downstream analysis
4. Adressed bug caused by subtracting with a float (would miss sites that should've been designated). Will now round the 1-"cut" value up to the number of decimal places in "cut". If you want a 1.00 cutoff, enter 0.99!
5. No longer compatible with tri-allelic loci. Only using bi-allelic for our purposes
6. Provided use the option to include a tsv with alternate chromosome names
7. Script will parse a complete VCF rather than require separae VCF and header files
8. Updated command line usage from arg.parse to sys. Still allows for "flags" through a dictionary system
9. Introduced more error handling
10. Added more thorough line-by-line annotations
11. Updated format to follow more of python script best practices

### Pull popmap list
```bash
for dir in /proj/sparrowhybridization/Pitaliae/working/Arianna/01_FlagstatExistingBAM/*/; do # ${dir} contains entire path

    popName=$(basename "$dir" _flagstats) # just the name of the specific directory is stored (e.g corsica)

    # Change p_montanus name to tree, for ease in sparrows.py script
    if [[ "$popName" == p_montanus ]]; # if the popuplation is p_montanus
    then
        popName="Tree" # then rename to "Tree"
    else
        popName=$(basename "$dir" _flagstats) # if not p_montanus, then keep name as is
    fi

    # Capitalize the first letter of the population
    popName="${popName^}" 

    # Pull the birdIDs and account for exceptions (e.g. changes made when generating VCF and true population of misc. files)
    for flagstat in ${dir}/*.flagstat; do # for each file in the directory that ends in .bam

        file=$(basename "$flagstat" .flagstat); # pull the basename, e.g. "K032_resorted_nodup_realigned"

        birdID=$(basename "$flagstat" .flagstat | cut -d "_" -f 1) # pull the birdID, e.g. "K032"

        # Account for exceptions!
        if [[ "$file" == Rimini* || "$file" == Lesina* ]]; # Rimini and Lesina populations follow a different format
        then
            birdID=${file} # pull the basename, e.g. "Rimini_112"
        else
            # When VCF file was generated, the "K" was removed from this sampleID
            if [[ "$birdID" == K034 ]];
            then
                birdID="034"
            else
                # This will reset the population for every misc. file (i.e 8934547, 8L19766, 8L52141, 8L52830, 8N05240, N33, M019, M036) to House
                if [[ "$birdID" == 8934547 ]];
                then
                    popName="House"
                else
                    # Overwrite population for misc. N33, should be Tree
                    if [[ "$birdID" == N33 ]];
                    then
                        popName="Tree"
                    fi
                fi
            fi
        fi

        # capitalize the first letter of the birdID
        birdID="${birdID^}" 
        
        # Don't include misc. M019 and M036 (remember their population is now called House)
        if [[ "$birdID" == M0* && "$popName" == House ]];
        then
            continue
        else
            echo -e "${birdID}\t${popName}" >> popmap.tsv # create a tab separated list of birdID and associated population
        fi

    done
done
```
Output: file called popmap.tsv which lists samples and associated populations in the following format: sample\tpopulation

### Pull list of chromosome name associations
```bash
# Create a tab separated list of the provided chromosome name and associated chromosome number, for ease of recognition in designation file output
cat  ../../Data/ReferenceGenome/GCA_036417665.1_bPasDom1.hap1_genomic.fna | grep ">" | tr -d ">" | sed 's/ Passer domesticus isolate bPasDom1 /\t/g' | sed 's/, whole genome shotgun sequence//g' | tr " " "_" | sed 's/chromosome_/chr/g' | sed 's/HAP1_SCAFFOLD_/scaffold/g' | sed 's/H_/scaffold/g' | sed 's/_unloc_1//g' | sed 's/_//g' > chromNames.tsv
```
Output: file called chromNames.tsv which lists genbank chrom ID and numerical chrom ID in the follow format: genbankID\tnumberID

### Create decompressed versions of the VCFs
```bash

cd /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs
mkdir VCFs_SansMaxMissing # For our purposes, will move forward with VCFs that didn't have a missingness filter applied

# Copy the VCFs over
cp ../../08_Filtering/SansMaxMissing/*.vcf.gz VCFs_SansMaxMissing

# Pull a list of samples we want to keep in our decompressed VCFs (House, Spanish, Tree, Corsica, Crete, Malta, Sicily)
cat popmap.tsv | grep 'Crete\|Corsica\|Sicily\|Malta\|House\|Spanish\|Tree' | cut -f 1 > samples_to_include

# Create a script (subsetVCFbySamples.sh) that will create decompressed VCFs of each of our filtered biallelic site vcf.gz files AND only include samples we want for our analysis (House, Spanish, Tree, Corsica, Crete, Malta, Sicily)
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts
mkdir 09_DesignateAlleles_scripts
cd 09_DesignateAlleles_scripts

# submitted batch job, 46822698
sbatch -A naiss2023-5-262 -p core -n 1 -t 23:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs/ -e ../../Job_logs/09_DesignateAlleles_logs/subsetVCFbySample.err -J subsetVCFsbySample --mail-user=ar4666al-s@student.lu.se --mail-type=ALL subsetVCFbySamples.shs
```
Outputs: within the VCFs_SansMaxMissing directory, 41 uncompressed VCF files with the following naming conventions -> samplesubset_[chromID]_SansMaxMissing.vcf

```bash
# Pull stats on number of variant sites per chromosome
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs/VCFs_SansMaxMissing # if not there already

for vcf in *.vcf ; do

    chromID=$(basename "${vcf}" .vcf | cut -d "_" -f 2 )

    #echo ${chromID}

    numberOfVariants=$(cat "${vcf}" | grep -v "#" | wc -l)

    #echo ${numberOfVariants}

    echo -e "${chromID}\t${numberOfVariants}" >> variantsPerChrom_sansMaxMiss.list

done

# Take output file to R in local computer and make graph of number of variants compared to chrom length, example usage!
Rscript vcf_stats.R -t ../variantsPerChrom_sansMaxMiss.list -s ../sequence_report.tsv 
```

## Run the python script
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles

mkdir Outputs
cd Outputs

mkdir Designations_VCFsansMaxMissing
cd Designations_VCFsansMaxMissing

# Create a temporary file that stores the paths for the vcf
for vcf in ../../Inputs/VCFs_SansMaxMissing/*.vcf; do
    echo ${vcf} >> subsetted_sansMaxMissing_VCF_paths
done

# Move to scripts directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Scripts/09_DesignateAlleles_scripts

# Create a bash script (designate_alleles.sh) to run the designation script (sparrows_Alamshahi.py)

# Submitted batch job 47163189
sbatch -A naiss2023-5-262 --array=1-41 --ntasks=1 -t 2-00:00:00 -D /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing -e ../../../Job_logs/09_DesignateAlleles_logs/designateAlleles.err -J designateAlleles --mail-user=ar4666al-s@student.lu.se --mail-type=ALL designate_alleles.sh

```
Outputs: 82 files per cutoff located in the Designations_VCFsansMaxMissing directory. Files follow this naming convention - [chromID]_inclMaxMiss.designated_cutoff_[cutoffLevel]_[sansSAHA/SAHA].tsv. 410 output files total.

## Create concatenated versions of the designation file based off cutoffs
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing

# Make directories for tsvs with SA HA designation
mkdir cutoff_0.80_sepByChrom_SAHA cutoff_0.85_sepByChrom_SAHA cutoff_0.90_sepByChrom_SAHA cutoff_0.95_sepByChrom_SAHA cutoff_0.99_sepByChrom_SAHA

# Make directories for tsvs with just A desgination
mkdir cutoff_0.80_sepByChrom_sansSAHA cutoff_0.85_sepByChrom_sansSAHA cutoff_0.90_sepByChrom_sansSAHA cutoff_0.95_sepByChrom_sansSAHA cutoff_0.99_sepByChrom_sansSAHA

# Move each tsv cutoff file into the corresponding cutoff directory
for tsv in *.tsv; do

    if [[ "$tsv" == *_cutoff_0.80_SAHA* ]]; 
    then
        mv ${tsv} cutoff_0.80_sepByChrom_SAHA
    fi

    if [[ "$tsv" == *_cutoff_0.80_sansSAHA* ]]; 
    then
        mv ${tsv} cutoff_0.80_sepByChrom_sansSAHA
    fi

    if [[ "$tsv" == *_cutoff_0.85_SAHA* ]]; 
    then
        mv ${tsv} cutoff_0.85_sepByChrom_SAHA
    fi

    if [[ "$tsv" == *_cutoff_0.85_sansSAHA* ]]; 
    then
        mv ${tsv} cutoff_0.85_sepByChrom_sansSAHA
    fi
    
    if [[ "$tsv" == *_cutoff_0.90_SAHA* ]]; 
    then
        mv ${tsv} cutoff_0.90_sepByChrom_SAHA
    fi
    
    if [[ "$tsv" == *_cutoff_0.90_sansSAHA* ]]; 
    then
        mv ${tsv} cutoff_0.90_sepByChrom_sansSAHA
    fi

    if [[ "$tsv" == *_cutoff_0.95_SAHA* ]]; 
    then
        mv ${tsv} cutoff_0.95_sepByChrom_SAHA
    fi

    if [[ "$tsv" == *_cutoff_0.95_sansSAHA* ]]; 
    then
        mv ${tsv} cutoff_0.95_sepByChrom_sansSAHA
    fi

    if [[ "$tsv" == *_cutoff_0.99_SAHA* ]]; 
    then
        mv ${tsv} cutoff_0.99_sepByChrom_SAHA
    fi

    if [[ "$tsv" == *_cutoff_0.99_sansSAHA* ]]; 
    then
        mv ${tsv} cutoff_0.99_sepByChrom_sansSAHA
    fi
  
done
```
Outputs: 10 subdirectories located in Designations_VCFsansMaxMissing. Directories are named cutoff_[cutoffLevel]_sepByChrom_[sansSAHA/SAHA] and the corresponding designation files are located in the directory. 41 designation files per directory!

```bash
# Create a concatenated version of the TSVs at each cuttoff
for dir in cutoff_*_sepByChrom*; do

    dirName=$(basename "$dir" .tsv | cut -d "_" -f 1-2,4 )

    for tsv in ${dir}/*.tsv; do
        cat ${tsv} >> "${dirName}.tsv" # Will include each of the headers from the independent designation files

    done
done 

# Only keep the first header, remove all the duplicate headers throughout the file
for comp_designation_file in *.tsv; do
    awk '/^#/ && !header_seen {print; header_seen=1; next} !/^#/' ${comp_designation_file} > "${comp_designation_file}_temp"

    # overwrite the file to reflect the temp file with just one header
    mv ${comp_designation_file}_temp "${comp_designation_file}"
done
```
Outputs: 10 .tsv files located in Designations_VCFsansMaxMissing. Files are named cutoff_[cutoffLevel]_[sansSAHA/SAHA].tsv. 

```bash
# Pull total number of site for each designation!
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing

for comp_designation_file in *tsv; do

    numberOfSites=$(cat "${comp_designation_file}" | grep -v "#" | wc -l)

    echo ${comp_designation_file} "number of sites: " ${numberOfSites}

done

# stdout
cutoff_0.80_SAHA.tsv number of sites:  493581
cutoff_0.80_sansSAHA.tsv number of sites:  493581
cutoff_0.85_SAHA.tsv number of sites:  394984
cutoff_0.85_sansSAHA.tsv number of sites:  394984
cutoff_0.90_SAHA.tsv number of sites:  250695
cutoff_0.90_sansSAHA.tsv number of sites:  250695
cutoff_0.95_SAHA.tsv number of sites:  170356
cutoff_0.95_sansSAHA.tsv number of sites:  170356
cutoff_0.99_SAHA.tsv number of sites:  137411
cutoff_0.99_sansSAHA.tsv number of sites:  137411
```

## Move to R on local computer and create some graphs looking at these .tsv designation files
``` bash
# Example usage of R script that will output plots for designation files!

 Rscript desFiles.R --tenRandomCorsicans yes --sansSAHA_0.80 ../newDesignationFiles/cutoff_0.80_sansSAHA.tsv --SAHA_0.80 ../newDesignationFiles/cutoff_0.80_SAHA.tsv --sansSAHA_0.85 ../newDesignationFiles/cutoff_0.85_sansSAHA.tsv --SAHA_0.85 ../newDesignationFiles/cutoff_0.85_SAHA.tsv --sansSAHA_0.90 ../newDesignationFiles/cutoff_0.90_sansSAHA.tsv --SAHA_0.90 ../newDesignationFiles/cutoff_0.90_SAHA.tsv --sansSAHA_0.95 ../newDesignationFiles/cutoff_0.95_sansSAHA.tsv --SAHA_0.95 ../newDesignationFiles/cutoff_0.95_SAHA.tsv --sansSAHA_0.99 ../newDesignationFiles/cutoff_0.99_sansSAHA.tsv --SAHA_0.99 ../newDesignationFiles/cutoff_0.99_SAHA.tsv -s ../sequence_report.tsv

```

# Check for intersection with bedtools

## Create subsetted VCF files that only include sites found at the 0.90 and 0.99 cutoffs
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 10_BEDTools
cd 10_BEDTools
mkdir VCFs_subsettedDerviedSites
cd VCFs_subsettedDerviedSites

# Request an interactive session, issues when running it as batch script (output saved to slurm.out), save time and run interactive, use print statements to monitor progress instead
interactive -A naiss2023-5-262 -n 20 -t 2:00:00
```

```bash
# For 0.99 cutofff

# Set the directories
designation_99_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing/cutoff_0.99_sepByChrom_SAHA"
vcf_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs/VCFs_SansMaxMissing"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools/VCFs_subsettedDerviedSites"

# Create a temporary directory to store a file with chrom ID and position from designation file
mkdir temp 

for des_file in "$designation_99_dir"/*.tsv; do # for each designation.tsv file

  chromID_des=$(basename "$des_file" | cut -d "_" -f 1) # pull the chromosome ID from the file name

  echo "Pulling chromosomes & positions from the $chromID_des designation file" # print to stdout to monitor progress

  awk 'NR>1 {split($1, a, "_"); print a[1], a[2]}' "$des_file" > "temp/${chromID_des}_positions.txt" # Skip the header, pull chromID and positionID from each line and save as tab separated temp file

done

# Now loop through the VCFs
for vcf_file in "$vcf_dir"/*.vcf; do

  chromID_vcf=$(basename "$vcf_file" | cut -d "_" -f 2) # pull the chromosome ID from the file name

  echo "Processing $chromID_vcf VCF file" # print to stdout to monitor progress

  if [ -f "temp/${chromID_vcf}_positions.txt" ]; then # check that the temp file with that position exists, it should - if it doesn't the we missed step

    awk 'BEGIN { while (getline < "temp/'"${chromID_vcf}"'_positions.txt") positions[$1][$2] = 1; } # reads lines from temp positions file into array
         /^#/ { print; next } # print out the header unaltered
         $1 in positions && $2 in positions[$1] { print }' "$vcf_file" > "$output_dir/${chromID_vcf}_subset_cutoff_0.99.vcf" # chrom ID and position from vcf exist in positions array

    echo "Created a subsetted VCF file for chromID $chromID_vcf" # stdout to monitor

  else
    echo "Can't find position file for $chromID_vcf" #stdout to monitor, means we missed step in creating temp directory

  fi

done

# Now remove the the temp directory before repeating process for 0.90 cutoff
rm -r temp

# Move the outputs to an associated directory
mkdir cutoff_0.99_subsettedVCFs
mv *vcf cutoff_0.99_subsettedVCFs/
```

```bash
# For 0.90 cutoff

# Set the directories
designation_90_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing/cutoff_0.90_sepByChrom_SAHA"
vcf_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs/VCFs_SansMaxMissing"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools/VCFs_subsettedDerviedSites"

# Create a temporary directory to store a file with chrom ID and position from designation file
mkdir temp 

for des_file in "$designation_90_dir"/*.tsv; do # for each designation.tsv file

  chromID_des=$(basename "$des_file" | cut -d "_" -f 1) # pull the chromosome ID from the file name

  echo "Pulling chromosomes & positions from the $chromID_des designation file" # print to stdout to monitor progress

  awk 'NR>1 {split($1, a, "_"); print a[1], a[2]}' "$des_file" > "temp/${chromID_des}_positions.txt" # Skip the header, pull chromID and positionID from each line and save as tab separated temp file

done

# Now loop through the VCFs
for vcf_file in "$vcf_dir"/*.vcf; do

  chromID_vcf=$(basename "$vcf_file" | cut -d "_" -f 2) # pull the chromosome ID from the file name

  echo "Processing $chromID_vcf VCF file" # print to stdout to monitor progress

  if [ -f "temp/${chromID_vcf}_positions.txt" ]; then # check that the temp file with that position exists, it should - if it doesn't the we missed step

    awk 'BEGIN { while (getline < "temp/'"${chromID_vcf}"'_positions.txt") positions[$1][$2] = 1; } # reads lines from temp positions file into array
         /^#/ { print; next } # print out the header unaltered
         $1 in positions && $2 in positions[$1] { print }' "$vcf_file" > "$output_dir/${chromID_vcf}_subset_cutoff_0.90.vcf" # chrom ID and position from vcf exist in positions array

    echo "Created a subsetted VCF file for chromID $chromID_vcf" # stdout to monitor

  else
    echo "Can't find position file for $chromID_vcf" #stdout to monitor, means we missed step in creating temp directory

  fi

done

# Now remove the the temp directory before repeating process for 0.90 cutoff
rm -r temp

# Move the outputs to an associated directory
mkdir cutoff_0.90_subsettedVCFs
mv *vcf cutoff_0.90_subsettedVCFs/
```
Outputs: 2 subdirectories named cutoff_0.90_subsettedVCFs and cutoff_0.99_subsettedVCFs each housing 41 subsetted VCF files named [chromID]_subset_cutoff_[cutoff].vcf

## Download the Refseq annotation file and update IDs to Genbank (our VCFs are generated with genbank IDs)
```bash
# Move to Data directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/Data/
mkdir Annotation
cd Annotation

# Download the genome annoation file (gff) from ncbi and copy to Annotation directory
# Download the sequence report from https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_036417665.1/ and copy to the Annotation directory

# Modify the tsv to only include the chromosome name (column 4), Genbank ID (column 7), and associated Refseq ID (column 10) 
cat sequence_report.tsv | grep -v "Assembly Accession" | cut -f 4,7,10 > genBank_refSeq_IDs.list # exclude the header and create tab sep list

# Create an updated .gff
# declares tab sep inputs & outputs, create an array where genbank ID is associated with refseq ID (from genBank_refSeq_IDs.list)
# Then checks if column 1 of gff exists in linked array, if yes replaces
awk 'BEGIN{FS=OFS="\t"} NR==FNR{linked[$3]=$2; next} $1 in linked{$1=linked[$1]} 1' genBank_refSeq_IDs.list genomic.gff > genomic_genbankID.gff
```
Output: gff file called genomic_genbankID.gff which features genBank IDs rather than refSeq and is located in the Annotation dir

## Create subsetted .gff files separeted by feature
```bash 
# Move back to the 10_BEDTools directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools

# Create and move to the gff_subsetted_byFeature directory
mkdir gff_subsetted_byFeature
cd gff_subsetted_byFeature

# Pull the list of features in the gtf file
cat ../../Data/Annotation/genomic_genbankID.gff | grep -v "#" | cut -f 3 | sort | uniq # cDNA_match, CDS, C_gene_segment, exon, gene, lnc_RNA, mRNA, ncRNA, pseudogene, region, rRNA, sequence_feature, snoRNA, snRNA, transcript, tRNA, V_gene_segment

# Create a gtf file for each feature we want: exons and gene (intergenic region and 10,00 basepairs from gene, use gene.gff)
awk '$3 == "exon"' ../../Data/Annotation/genomic_genbankID.gff > exon.gff # 858,118 lines, will only pull lines where "exon" is listed as feature
awk '$3 == "gene"' ../../Data/Annotation/genomic_genbankID.gff > gene.gff # 31,174 lines
```
Outputs: 2 files in the gff_subsetted_byFeature directory following the format [geneFeature].[fileType(gtf/gff)]

## Generate the VCF intersection files
```bash
# Request an interactive environment, issues with output saving into slurm file, for efficiency just do interative
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load BEDTools/2.31.1

# Move back to the 10_BEDTools directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools

# Create directories to store the intersection files
mkdir VCF_cutoff_0.90_intersections VCF_cutoff_0.99_intersections

# Genes & exons
for vcf_dir in VCFs_subsettedDerviedSites/cutoff_*_subsettedVCFs ; do # there are two dirs storing VCFs, 0.90 and 0.99 cutoff

    for vcf in ${vcf_dir}/*vcf ; do # for the vcf files in those dirs

        for feature_file in gff_subsetted_byFeature/*.g*f ; do

            chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name

            cutoff=$(basename "$vcf" .vcf | cut -d "_" -f 4) # pull the designation cutoff from the vcf file name

            featureType=$(basename "$feature_file" .g*f | cut -d "." -f 1) # pull the annotation feature type from the file name

            #echo ${featureType}

            bedtools intersect -header -a ${vcf} -b ${feature_file} -u > "VCF_cutoff_${cutoff}_intersections/${chromID}_cutoff_${cutoff}_${featureType}_intersection.vcf" # only pull unique overlaps (-u) and maintain the vcf header

        done

    done
    
done

# Intergenic regions
for vcf_dir in VCFs_subsettedDerviedSites/cutoff_*_subsettedVCFs ; do # there are two dirs storing VCFs, 0.90 and 0.99 cutoff

    for vcf in ${vcf_dir}/*vcf ; do

        chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name

        cutoff=$(basename "$vcf" .vcf | cut -d "_" -f 4) # pull the designation cutoff from the vcf file name
        
        bedtools intersect -header -a ${vcf} -b gff_subsetted_byFeature/gene.gff -v > "VCF_cutoff_${cutoff}_intersections/${chromID}_cutoff_${cutoff}_intergenic_intersection.vcf" # -v finds features that don't overlap

    done
    
done

# Genes & up to 10,000 basepairs away from gene (features like enhancers and promoters can be very vary away, but setting cutoff of 100,000 will pull WAY too much, will capture enough with 10,000 bps )

for vcf_dir in VCFs_subsettedDerviedSites/cutoff_*_subsettedVCFs ; do # there are two dirs storing VCFs, 0.90 and 0.99 cutoff

    for vcf in ${vcf_dir}/*vcf ; do

        chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name

        cutoff=$(basename "$vcf" .vcf | cut -d "_" -f 4) # pull the designation cutoff from the vcf file name
        
        bedtools window -header -a ${vcf} -b gff_subsetted_byFeature/gene.gff -u -w 10000 > "VCF_cutoff_${cutoff}_intersections/${chromID}_cutoff_${cutoff}_geneAndNearGene_10000bpWindow_intersection.vcf" # window and -w to find genes sites and near gene sites in file

    done
    
done

```

### Sort the VCF intersection files
```bash
# Move back to the 10_BEDTools directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools

# Now move the the 0.90 VCF intersection files
cd VCF_cutoff_0.90_intersections

mkdir exon_sites gene_sites intergenic_sites geneAndNearGene10000_sites

for vcf in *vcf; do

    vcf_basename=$(basename ${vcf} .vcf)

    if [[ "$vcf_basename" == *exon* ]]; then
      mv "$vcf" exon_sites

    elif [[ "$vcf_basename" == *10000bpWindow* ]]; then
    mv "$vcf" geneAndNearGene10000_sites

    elif [[ "$vcf_basename" == *intergenic* ]]; then
    mv "$vcf" intergenic_sites
    
    elif [[ "$vcf_basename" == *_gene_* ]]; then
    mv "$vcf" gene_sites

    fi

done

# Repeat the sorting process for files in the VCF_cutoff_0.99_intersections directory
```

### Now generate intersection file for sites near gene
```bash
# Now pull just nearby!
for vcf_intersection_dir in VCF_cutoff_* ; do # there are two dirs storing VCFs, 0.90 and 0.99 cutoff

    for gene_intersection_vcf in ${vcf_intersection_dir}/gene_sites/*vcf ; do

        chromIDgeneIntersection=$(basename "$gene_intersection_vcf" .vcf | cut -d "_" -f 1) # Pull chrom name from the gene intersection file name
        cutoffGeneIntersection=$(basename "$gene_intersection_vcf" .vcf | cut -d "_" -f 3) # Pull cutoff from the gene intersection file name

        for gene_ANDnearby_intersection_vcf in ${vcf_intersection_dir}/geneAndNearGene10000_sites/${chromIDgeneIntersection}*${cutoffGeneIntersection}*vcf ; do # find gene&nearby file that matches chrom name and cutoff of gene file

            bedtools intersect -header -a ${gene_ANDnearby_intersection_vcf} -b ${gene_intersection_vcf} -v > "VCF_cutoff_${cutoffGeneIntersection}_intersections/${chromIDgeneIntersection}_cutoff_${cutoffGeneIntersection}_nearGene_intersection.vcf" # -v flag will print out any site present in gene & nearby that aren't present in gene -> so just nearby!!


        done

    done
    
done
```

### Sort the near_gene intersections
```bash
# sort the near gene intersection files!
# Move back to the 10_BEDTools directory if not there already
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools

# Now move the the 0.90 VCF intersection files
cd VCF_cutoff_0.90_intersections

# Make a directory for nearGene sites and move associated files there!
mkdir nearGene_sites
mv *vcf nearGene_sites/

# Repeat the sorting process for files in the VCF_cutoff_0.99_intersections directory
```
Outputs: two directories named VCF_cutoff_[cutoff]_intersections. Within each directory are 6 subdirectories named after features called exon_sites, geneAndNearGene10000_sites, gene_sites, intergenic_sites, and nearGene_sites. Each subdirectory contains 41 VCF files named [chromID]_cutoff_[cutoff]_[feature]_intersection.vcf

# GO analysis - SCRAPPED!

## Create tab separated list of Gene_IDs and associated GO_IDs
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 11_GOanalysis
cd 11_GOanalysis

# Download the GCF_036417665.1-RS_2024_04_gene_ontology.gaf.gz file from https://ftp.ncbi.nlm.nih.gov/genomes/all/annotation_releases/48849/GCF_036417665.1-RS_2024_04/
# Unzip and move file to the 11_GOanalysis directory, file called GCF_036417665.1-RS_2024_04_gene_ontology.gaf

# Use awk to parse the GAF file to get tab sep list of gene IDs and GO ids
for gaf in GCF_036417665.1-RS_2024_04_gene_ontology.gaf; do

    awk -F'\t' '
    !/^!/ { # ignore header lines, they start with "!"
        gene_id = $2 # gene IDs are located in column 2
        go_id = $5 # GO IDs located in column 3, format GO:0016746

        # Store GO IDs in an associative array (index by gene IDs)
        if (gene_id in gene_go) {
            gene_go[gene_id] = gene_go[gene_id] "," go_id # if there are multiple GO IDs associated with the same gene ID, comma sep the GO IDs (GO:0016746, GO:0016745)
        } else {
            gene_go[gene_id] = go_id # otherwise just keep as one GO ID associated with one Gene ID
        }
    }

    END {
        for (gene_id in gene_go) {
            print gene_id "\t" gene_go[gene_id] # tab sep output, [geneID]\t[GO:ID]
        }
    }
    ' "$gaf" > "geneID_goID.tsv"

done

# Check number of unique gene IDs in GAF
cat GCF_036417665.1-RS_2024_04_gene_ontology.gaf | grep -v "\!" | cut -f 2 | sort | uniq | wc -l # 16024 gene IDS

# Compare to number of lines present in geneID_goID.tsv
cat geneID_goID.tsv | wc -l # 16024 gene IDS, exact match!

# Add a header to the top of the file,
echo -e "GeneID\tGO_ID" > annotationHeader
cat annotationHeader geneID_goID.tsv > temp.tsv 
mv temp.tsv geneID_goID.tsv

rm annotationHeader # remove this temporary file
```
Output: 1 file called geneID_goID.tsv located in the 11_GOanalysis directory

## Create a file of outlier genes
### Subset the 0.99 designation files based on SD or HD site designation
```bash
# Subset the 0.99 (just run for 0.99 for now) designation files based on SD or HD site designation
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis
mkdir designationFiles_sep_SD_HD
cd designationFiles_sep_SD_HD

# Request an interactive session!
interactive -A naiss2023-5-262 -n 10 -t 1:00:00

# Directory containing the files
designation_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing/cutoff_0.99_sepByChrom_SAHA" # very long, save into environmental variable!
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/designationFiles_sep_SD_HD" # also set this to ensure output goes where we want

# Iterate over each file in the input directory
for des_file in "$designation_dir"/*.tsv; do

    file_basename=$(basename ${des_file} .tsv | cut -d "_" -f 1,3-4 ) # e.g. CM071462.1_cutoff_0.99

    sd_des_output="$output_dir/${file_basename}_SD.tsv" # create variables to store output file names, will be shorter to call in further down
    hd_des_output="$output_dir/${file_basename}_HD.tsv"
    > "$sd_des_output" # create empty files for output to append to
    > "$hd_des_output"
    
    while IFS= read -r line; do # read designation file line by line

        if [[ "$line" =~ ^# ]]; then # skip the header! Starts with "#"
        continue
        fi
        
        site_des_col=$(echo "$line" | cut -f 6) # pull the site designation, in column 6
        
        
        if [[ "$site_des_col" == "SD" ]]; then # If site is designated SD
        echo "$line" >> "$sd_des_output" # then write that line to SD output file

        elif [[ "$site_des_col" == "HD" ]]; then # if site is designated HD
        echo "$line" >> "$hd_des_output" # then write that line to HD output file
        fi
    done < "$des_file"

done # took about 11 minutes

# check that SD lines + HD lines = # lines in orginal file!
```
Output: 82 files located in the designationFiles_sep_SD_HD directory! Files are named [chromID]_cutoff_0.99_[siteDes].tsv

### Generate subsetted VCF files separated by SD and HD designation (0.99 cutoff)
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis
mkdir VCFs_subsetted_SD_HD

# This uses the same baseline code as we used in previous steps!

# Request interactive environment
interactive -A naiss2023-5-262 -n 20 -t 2:00:00

designation_99_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/designationFiles_sep_SD_HD"
vcf_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Inputs/VCFs_SansMaxMissing"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD"

# Create a temporary directory to store a file with chrom ID and position from HD designation files
mkdir temp 


for des_file in "$designation_99_dir"/*_HD.tsv; do # for each HD designation.tsv file

  chromID_des=$(basename "$des_file" | cut -d "_" -f 1) # pull the chromosome ID from the file name

  echo "Pulling chromosomes & positions from the $chromID_des designation file" # print to stdout to monitor progress

  awk '{split($1, a, "_"); print a[1], a[2]}' "$des_file" > "temp/${chromID_des}_positions.txt" # No header to skip, pull chromID and positionID from each line and save as tab separated temp file

done

# Now loop through the VCFs
for vcf_file in "$vcf_dir"/*.vcf; do

  chromID_vcf=$(basename "$vcf_file" | cut -d "_" -f 2) # pull the chromosome ID from the file name

  echo "Processing $chromID_vcf VCF file" # print to stdout to monitor progress

  if [ -f "temp/${chromID_vcf}_positions.txt" ]; then # check that the temp file with that position exists, it should - if it doesn't the we missed step

    awk 'BEGIN { while (getline < "temp/'"${chromID_vcf}"'_positions.txt") positions[$1][$2] = 1; } # reads lines from temp positions file into array
         /^#/ { print; next } # print out the header unaltered
         $1 in positions && $2 in positions[$1] { print }' "$vcf_file" > "$output_dir/${chromID_vcf}_subset_cutoff_0.99_HD.vcf" # chrom ID and position from vcf exist in positions array

    echo "Created a subsetted VCF file for chromID $chromID_vcf" # stdout to monitor

  else
    echo "Can't find position file for $chromID_vcf" #stdout to monitor, means we missed step in creating temp directory

  fi

done

# Now remove the the temp directory before repeating process for SD designated
rm -r temp

# Sort the output
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD
mkdir VCFs_subsetted_HD
mv *vcf VCFs_subsetted_HD/ # move all the files there

# Create a temporary directory to store a file with chrom ID and position from SD designation files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis # move back first
mkdir temp 

for des_file in "$designation_99_dir"/*_SD.tsv; do # for each HD designation.tsv file

  chromID_des=$(basename "$des_file" | cut -d "_" -f 1) # pull the chromosome ID from the file name

  echo "Pulling chromosomes & positions from the $chromID_des designation file" # print to stdout to monitor progress

  awk '{split($1, a, "_"); print a[1], a[2]}' "$des_file" > "temp/${chromID_des}_positions.txt" # No header to skip, pull chromID and positionID from each line and save as tab separated temp file

done

# Now loop through the VCFs
for vcf_file in "$vcf_dir"/*.vcf; do

  chromID_vcf=$(basename "$vcf_file" | cut -d "_" -f 2) # pull the chromosome ID from the file name

  echo "Processing $chromID_vcf VCF file" # print to stdout to monitor progress

  if [ -f "temp/${chromID_vcf}_positions.txt" ]; then # check that the temp file with that position exists, it should - if it doesn't the we missed step

    awk 'BEGIN { while (getline < "temp/'"${chromID_vcf}"'_positions.txt") positions[$1][$2] = 1; } # reads lines from temp positions file into array
         /^#/ { print; next } # print out the header unaltered
         $1 in positions && $2 in positions[$1] { print }' "$vcf_file" > "$output_dir/${chromID_vcf}_subset_cutoff_0.99_SD.vcf" # chrom ID and position from vcf exist in positions array

    echo "Created a subsetted VCF file for chromID $chromID_vcf" # stdout to monitor

  else
    echo "Can't find position file for $chromID_vcf" #stdout to monitor, means we missed step in creating temp directory

  fi

done

# Now remove the the temp directory 
rm -r temp

# Sort the output
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD
mkdir VCFs_subsetted_SD
mv *vcf VCFs_subsetted_SD/ # move all the vcf files there

# Compare line counts to make sure they add up!
cat VCFs_subsetted_HD/CM071426.1_subset_cutoff_0.99_HD.vcf | grep -v "#" | wc -l # 720
cat VCFs_subsetted_SD/CM071426.1_subset_cutoff_0.99_SD.vcf | grep -v "#" | wc -l # 1557
cat ../../10_BEDTools/VCFs_subsettedDerviedSites/cutoff_0.99_subsettedVCFs/CM071426.1_subset_cutoff_0.99.vcf | grep -v "#" | wc -l # 2277, this is VCF subsetted by derived sites not split up by SD and HD! 720 + 1557 = 2277
```
Output: two subdirectories called VCFs_subsetted_HD and VCFs_subsetted_SD located in the 11_GOanalysis directory. Each subdirectory contains 41 vcf files that follow this naming convention: [chromID]_subset_cutoff_0.99_[HD/SD].vcf! Recall that these vcfs are SUBSETS, they only include sites that are also present in the associated designation (0.99 cutoff) file!

### Generate exon and gene gff intersection files based on subsetted designation files (HD and SD)

```bash
# Move back to 11_GOanalysis
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis

cp -r ../10_BEDTools/gff_subsetted_byFeature/ . # copy the directory with exon.gff and gene.gff file!

mkdir GFF_cutoff_0.99_SD_HD_intersections # directory to store intersections

# Request an interactive environment
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load BEDTools/2.31.1

# Create gff intersection file for genes and exons, separated by SD and HD
for vcf_dir in VCFs_subsetted_SD_HD/VCFs_subsetted_*D ; do # there are two dirs storing VCFs, separated by SD and HD

    for vcf in ${vcf_dir}/*vcf ; do

        for feature_file in gff_subsetted_byFeature/*.gff ; do

            chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name

            siteType=$(basename "$vcf" .vcf | cut -d "_" -f 5) # pull the site type, SD or HD

            featureType=$(basename "$feature_file" .g*f | cut -d "." -f 1) # pull the annotation feature type from the file name

            bedtools intersect -header -a ${feature_file} -b ${vcf} -u > "GFF_cutoff_0.99_SD_HD_intersections/${chromID}_cutoff_0.99_${featureType}_${siteType}_intersection.gff" # only pull unique overlaps

        done

    done
    
done
```

```bash
# Sort the GFF intersection files

# Move to to the 11_GOanalysis directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/GFF_cutoff_0.99_SD_HD_intersections

mkdir exon_sites_HD gene_sites_HD exon_sites_SD gene_sites_SD

for gff in *gff; do

    gff_basename=$(basename ${gff} .gff)

    if [[ "$gff_basename" == *exon_HD* ]]; then
      mv "$gff" exon_sites_HD

    elif [[ "$gff_basename" == *gene_HD* ]]; then
    mv "$gff" gene_sites_HD

    elif [[ "$gff_basename" == *exon_SD* ]]; then
    mv "$gff" exon_sites_SD
    
    elif [[ "$gff_basename" == *gene_SD* ]]; then
    mv "$gff" gene_sites_SD

    fi

done
```
Output: 4 subdirectories called exon_sites_HD, exon_sites_SD, gene_sites_HD, gene_sites_SD located within the GFF_cutoff_0.99_SD_HD_intersections subdirectory. Within each [feature]_sites_[designation] subdirectory are 41 gff intersection files! They have been intersection with a VCF that contains SD or HD sites at the 0.99 cutoff and follow this naming convention: [chromID]_cutoff_0.99_[feature]_[siteDesignation]_intersection.gff


### Pull the outlier genes into a file
```bash
# Move into this directory if not already there!
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis

mkdir outlier_GeneID_files
cd outlier_GeneID_files
mkdir outlier_GeneID_files_SD outlier_GeneID_files_HD

# set environmental variables instead of referencing absolute path each time
subsetted_gff_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/GFF_cutoff_0.99_SD_HD_intersections"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/outlier_GeneID_files"

for dir in in ${subsetted_gff_dir}/gene_sites_*D; do # two directories gene_sites_HD, gene_sites_SD

    for gff in ${dir}/*.gff; do

        file_name=$(basename "$gff" .gff) # pull the basename, e.g. CM071426.1_cutoff_0.99_gene_SD_intersection
        site_des=$(basename "$dir" | cut -d "_" -f 3) # will just pull the SD or HD from end of directory name

        cat ${gff} | cut -f 9 | cut -d ";" -f 2 | cut -d ":" -f 2 >> "$output_dir/outlier_GeneID_files_${site_des}/${file_name}_outlierGeneID.tsv"

    done

done

# Add a header to each file
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis
echo "geneID" > candidatesHeader # create a header file

outlier_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/outlier_GeneID_files" # set the output directory, the dir where the tsvs are currently

for dir in ${outlier_dir}/outlier_GeneID_files_*D; do

    for tsv in ${dir}/*.tsv; do

        cat candidatesHeader ${tsv} > "$dir/temp.tsv" # create a temp file

        mv "${dir}/temp.tsv" "$tsv" # override the current file with the temp file (using the current file name)

    done

done

rm candidatesHeader # remove this unecessary file
```
Outputs: two subdirectories called outlier_GeneID_files_HD and outlier_GeneID_files_SD within the outlier_GeneID_files subdirectory. Within each of the two subdirectories are 41 files following this naming convention: [chromID]_cutoff_0.99_gene_[siteDesignation]_intersection_outlierGeneID.tsv. The file is a list with gene IDs fo HD/SD sites that fall within genes at the 0.99 cutoff


## Run using Kalle's GO_term scripts
```bash
# Made some adjustments to existing GO_terms (BP = biological process & MF = molecular function) script!
# re-wrote package installations as some packages require BiocManager
# change emptying of objects from objects() to rm(list = objects())
# Replaced comment that specified unecessary japponicus specific header, replaced with comment that explains actual necessary header


# Issues with R package installation on server (even after loading R/4.2.1)
# When packages are not installed on local computer, R script effectively installs them from command line so it does work.
# Move to local computer to run this analysis (in the spirit of efficiency)

# Local comp
cd ~/BINP37
mkdir 11_GOanalysis
cd 11_GOanalysis # copy Kalle's updated GSEA_script here
mkdir GO_output

# Copy the necessary files
scp arianna2@rackham.uppmax.uu.se:/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/geneID_goID.tsv .
scp -r arianna2@rackham.uppmax.uu.se:/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/outlier_GeneID_files .

# Run the script from the GO_analysis_output directory!

for outlier_tsv in outlier_GeneID_files/outlier_GeneID_files_*D/*tsv; do

    Rscript GSEA_script_updated.R geneID_goID.tsv ${outlier_tsv}

done # will output into the the associated outlier_GeneID_files_*D directory, sort files post-productions

# Move to folder with outputs
cd GO_output
mkdir HD_output SD_output

# Sort outputs by designation (SD or HD)
for candidate_dir in ../outlier_GeneID_files/outlier_GeneID_files_*D; do

    site_des=$(basename "$candidate_dir" | cut -d "_" -f 4) # pull the site designation from the end of the dir name

    for output_file in ${candidate_dir}/*GSEA*; do

        mv ${output_file} "./${site_des}_output"

    done

done

# Further sort output by go term, BP or MF
for output_dir in *D_output; do


    for output_file in ${output_dir}/*GSEA*; do

        go_term=$(basename "$output_file" | cut -d "_" -f 7 | cut -d "." -f 3) # isolates the BP or MF from file name

        # Path for where the file should be moved to
        term_output_dir="${output_dir}/${go_term}_output"

        # If the directory doesn't already exist, make one
        mkdir -p "$term_output_dir" # -p will prevent warning messages about being unable to make dir bc it already exists from being printed

        mv "$output_file" "$term_output_dir" # move file to associated term sub directory

    done


done

# Further sort output by chromosome!
for output_dir in *D_output/*_output; do


    for output_file in ${output_dir}/*GSEA*; do

        chromID=$(basename "$output_file" | cut -d "_" -f 1) # isolates the chromID from file name

        # Path for where the file should be moved to
        chrom_output_dir="${output_dir}/${chromID}_output"

        # If the directory doesn't already exist, make one
        mkdir -p "$chrom_output_dir" # -p will prevent warning messages about being unable to make dir bc it already exists from being printed

        mv "$output_file" "$chrom_output_dir" # move file to associated chromosome sub directory

    done


done

```
Outputs: subdirectories named [chromID]_output within parent directories MF_output and BP_output nested within parent directories named SD_output and HD_output all within directory called GO_output. There are 12 chromosome subdirectories within HD/BP, 10 in HD/MF, 15 SD/BP, and 13 in SD/MF. Not every chromosome has a directory because some don't have candidate genes and some that have few genes might be able to produce results with BP for example but not MF. 

## For GO analysis, would be more interesting to look at HD sites, SD sites, and the intersesction between the two! And to re-run with all autosomes together and Z separate! Ran out of time, but this is a good starting point for analysis


# Linkage pruning, then re-run some stats in R

## Generate a list of prune.in sites!
```bash
# Create a new directory for pruninig VCFs and subsequent analysis
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 12_LinkagePruning
cd 12_LinkagePruning

mkdir InputVCFs
cd InputVCFs

# Copy VCFs that have been subsetted based on designation file!
cp -r ../../10_BEDTools/VCFs_subsettedDerviedSites/cutoff_0.9* . #cutoff_0.90_subsettedVCFs and cutoff_0.99_subsettedVCFs subdirectories!

# Move back to 12_LinkagePruning!
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning

mkdir VCF_subsettedByDes_pruning # create a directory to store PLINK output

# Request interactive
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load plink/1.90b4.9

# create environmental variables to store paths
VCFs_subsetted="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/InputVCFs"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning"

for vcf_dir in ${VCFs_subsetted}/cutoff_0.9*_subsettedVCFs ; do # there are two dirs storing VCFs, separated by 0.99 and 0.90

    for vcf in ${vcf_dir}/*vcf ; do

        chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name
        cutoff=$(basename "$vcf" .vcf | cut -d "_" -f 4) # pull the cutoff from the vcf file name


        # Allow extra chromosome - we have a Z chromosome and PLINK was designed for humans
        # Set missing var IDS - so PLINK will use chromosome ID and position as the unique ID (chromID_pos, this format will match nicely with des file)
        # indep pairwise - 450 = window (Kb,) 10 = window step size (bp), 0.1 = r2 value, linkage level we'll accept (prune if > 0.1)
        plink --vcf ${vcf} --allow-extra-chr --set-missing-var-ids @_# --indep-pairwise 450 10 0.1 --out "${output_dir}/${chromID}_cutoff_${cutoff}_pruned"

    done
    
done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning
mkdir pruned_0.99_cutoff pruned_0.90_cutoff

for file in ./*; do

    if [[ "$file" == *cutoff_0.99* ]]; then
      mv "$file" pruned_0.99_cutoff # 160 files located here

    elif [[ "$file" == *cutoff_0.90* ]]; then
    mv "$file" pruned_0.90_cutoff # 162 files located here, because more empty vcf files which triggered temp file making

    fi

done

# Create concatenated version of pruned in list for 0.90 and 0.99 cutoffs
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning

cat pruned_0.90_cutoff/*prune.in > prune_in_0.90_cutoff # list of [chromID]_[positions] to keep for 0.90 cutoff, 523 sites
cat pruned_0.99_cutoff/*prune.in > prune_in_0.99_cutoff # list of [chromID]_[positions] to keep for 0.99 cutoff, 267 sites
```
Outputs: pruned_0.90_cutoff pruned_0.99_cutoff directories wich store prune.in, prune.out, .log, .nosex outputs AND the prune_in_0.90_cutoff prune_in_0.99_cutoff lists which shows all the [chromID]_[positions] to keep after pruning!

## Subset the designation files based on prune.in sites

```bash

# Subset the 0.99 and 0.90 designation files for prune.in sites
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning
mkdir Pruned_designation_files

# Request an interactive session!
interactive -A naiss2023-5-262 -n 20 -t 2:00:00

# Directory containing the files, 0.99 cutoff
designation_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing" # very long, save into environmental variable!
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files" # also set this to ensure output goes where we want
prune_in_file="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff"

# Iterate over each file in the input directory
for des_dir in "$designation_dir"/cutoff_0.99_sepByChrom_*; do # SAHA and sansSAHA

    for des_file in "$des_dir"/*.tsv; do


        file_basename=$(basename ${des_file} .tsv | cut -d "_" -f 1,3-5 ) # e.g. CM071462.1_cutoff_0.99_sansSAHA


        pruned_output="${output_dir}/${file_basename}_pruned.tsv" # create variables to store output file names, will be shorter to call in further down
        > "$pruned_output" # initiate empty files for output to append to
        
        while IFS= read -r line; do # read designation file line by line

            if [[ "$line" =~ ^# ]]; then # If header line, starts with "#"
            echo "$line" >> "$pruned_output" # keep the header in new output file, but don't scan header for chromID_position
            continue
            fi
            
            locus_col=$(echo "$line" | cut -f 1 | cut -d "_" -f 1-2 ) # pull the locus from column 1, then isolate [chromID]_[position]
            
            if grep -qFx "$locus_col" "$prune_in_file"; then # checks the [chromID]_[position] from the site des file against the list of retained sites post-pruning

            echo "$line" >> "$pruned_output" # if exists, then print designation file line to the output

            fi
        done < "$des_file"

    done

done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files

mkdir pruned_desFiles_sepByChrom_0.99_cutoff
mv *tsv pruned_desFiles_sepByChrom_0.99_cutoff


# Repeat for 0.90 ccutoff, directory containing the files - 0.90 cutoff
# Only need to reset this!
prune_in_file="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.90_cutoff"


# Iterate over each file in the input directory
for des_dir in "$designation_dir"/cutoff_0.90_sepByChrom_*; do # SAHA and sansSAHA

    for des_file in "$des_dir"/*.tsv; do


        file_basename=$(basename ${des_file} .tsv | cut -d "_" -f 1,3-5 ) # e.g. CM071462.1_cutoff_0.99_sansSAHA


        pruned_output="${output_dir}/${file_basename}_pruned.tsv" # create variables to store output file names, will be shorter to call in further down
        > "$pruned_output" # initiate empty files for output to append to
        
        while IFS= read -r line; do # read designation file line by line

            if [[ "$line" =~ ^# ]]; then # If header line, starts with "#"
            echo "$line" >> "$pruned_output" # keep the header in new output file, but don't scan header for chromID_position
            continue
            fi
            
            locus_col=$(echo "$line" | cut -f 1 | cut -d "_" -f 1-2 ) # pull the locus from column 1, then isolate [chromID]_[position]
            
            if grep -qFx "$locus_col" "$prune_in_file"; then # checks the [chromID]_[position] from the site des file against the list of retained sites post-pruning

            echo "$line" >> "$pruned_output"  # if exists, then print designation file line to the output


            fi
        done < "$des_file"

    done

done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files

mkdir pruned_desFiles_sepByChrom_0.90_cutoff
mv *tsv pruned_desFiles_sepByChrom_0.90_cutoff

# Create a concatenated version of the TSVs at each cuttoff (so all chroms in 1), reuse code from above
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files # if not there already

for dir in pruned_desFiles_sepByChrom_*_cutoff; do

    cutoff=$(basename "$dir" | cut -d "_" -f 4 )

    for SAHA_tsv in ${dir}/*_SAHA_pruned.tsv; do

        # Will include each of the headers from the independent designation files
        cat ${SAHA_tsv} >> "./pruned_cutoff_${cutoff}_SAHA.tsv" # 0.90 - 564 lines incl headers, 0.99 - 308 lines incl headers

    done

    for sansSAHA_tsv in ${dir}/*_sansSAHA_pruned.tsv; do

        cat ${sansSAHA_tsv} >> "./pruned_cutoff_${cutoff}_sansSAHA.tsv" # 0.90 - 564 lines incl headers, 0.99 - 308 lines incl headers

    done
done 


# Only keep the first header, remove all the duplicate headers throughout the file
for comp_designation_file in *.tsv; do
    awk '/^#/ && !header_seen {print; header_seen=1; next} !/^#/' ${comp_designation_file} > "${comp_designation_file}_temp"

    # overwrite the file to reflect the temp file with just one header
    mv ${comp_designation_file}_temp "${comp_designation_file}"
done
```
Outputs: 4 output files (1 for each cutoff SAHA/sansSAHA combo in the Pruned_designation_files directory). Files are named pruned_cutoff_[cutoffLevel]_[sansSAHA/SAHA].tsv. 

## Copy the pruned designation files to the local computer and re-run desFiles.R and tests.R

# Heatmaps

## Create a pruned version of VCF files to use in visualization!

```bash
# Move to working directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna

# Will mostly be used to store inputs as we will run the R scripts on local computer!
mkdir 13_Heatmaps # make a symbolic directory called 13_Heatmaps! Won't store anything here, just to keep track of steps
cd 13_Heatmaps

# Use the prune.in sites list we used previously to create a Regions file for bcftools! chrom\tposition
cat ../12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.90_cutoff | tr "_" "\t" > prune_in_0.90_cutoff_bcftoolsFormat
cat ../12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff | tr "_" "\t" > prune_in_0.99_cutoff_bcftoolsFormat

# bgzip the subsetted VCFs
mkdir Subsetted_VCFs_bgzipped

interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load bcftools/1.14

vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/10_BEDTools/VCFs_subsettedDerviedSites"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/13_Heatmaps/Subsetted_VCFs_bgzipped"

for vcf_dir in "$vcf_parent_dir"/cutoff_0.9*_subsettedVCFs; do

    vcf_dir_name=$(basename "$vcf_dir" ) # pulls just the name, not whole path

    for vcf in "$vcf_dir"/*vcf; do
        
        vcf_name=$(basename "$vcf" ) # keep the .vcf ending!

        sub_output_dir="${output_dir}/${vcf_dir_name}_bgzipped"

        mkdir -p ${sub_output_dir} # will not override existing dir
    
        bcftools view ${vcf} -Oz --threads 20 -o "${sub_output_dir}/${vcf_name}.gz" 

    done

done # took ~2 minutes
```
Outputs: 41 bgzipped vcf files (subsetted by 0.90 and 0.99 cutoff) per cutoff! Located in Subsetted_VCFs_bgzipped/cutoff_[cutoff]_subsettedVCFs_bgzipped/[.vcf files]

```bash
# Now index the compressed subsetted vcfs
vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/13_Heatmaps/Subsetted_VCFs_bgzipped"

for vcf_dir in "$vcf_parent_dir"/cutoff_0.9*_subsettedVCFs_bgzipped; do

    for vcf in "$vcf_dir"/*vcf.gz; do
        
        bcftools index ${vcf}

    done

done # took ~2 minutes

```
Outputs: each of the vcf.gz files that were outputted from above should be accompanied by a .csi file, 82 in total (41 for each cutoff)

```bash
# Move to 13_Heatmaps dir
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/13_Heatmaps
mkdir Subsetted_VCFs_linkagePruned

# Now run bcftools to only keep specific site

vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/13_Heatmaps/Subsetted_VCFs_bgzipped"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/13_Heatmaps/Subsetted_VCFs_linkagePruned"


for vcf_dir in "$vcf_parent_dir"/cutoff_0.9*_subsettedVCFs_bgzipped; do

    vcf_dir_name=$(basename "$vcf_dir" ) # pulls just the name, not whole path
    cutoff=$(basename "$vcf_dir" | cut -d "_" -f 2 )

    for vcf in "$vcf_dir"/*vcf.gz; do
        
        vcf_name=$(basename "$vcf" .vcf.gz ) # file name without .vcf.gz

        sub_output_dir="${output_dir}/pruned_${vcf_dir_name}"

        mkdir -p ${sub_output_dir} # will not override existing dir

        # use the cutoff associated prune.in list
        bcftools view ${vcf} -R "prune_in_${cutoff}_cutoff_bcftoolsFormat" -Oz -o "${sub_output_dir}/${vcf_name}_pruned.vcf.gz" 
    

    done

done # took ~1 minute

# Check that total number of sites left matches number of prune.in sites
zcat Subsetted_VCFs_linkagePruned/pruned_cutoff_0.90_subsettedVCFs_bgzipped/*_subset_cutoff_0.90_pruned.vcf.gz | grep -v "#" | wc -l # 523, matches
zcat Subsetted_VCFs_linkagePruned/pruned_cutoff_0.99_subsettedVCFs_bgzipped/*_subset_cutoff_0.99_pruned.vcf.gz | grep -v "#" | wc -l # 267, matches

# Copy the linkage pruned subsetted VCFs to local directory and use for heatmaps!


for vcf in ls ../../Rscripts/genotypePlot/VCF_inputs/VCFs_subsettedDerviedSites/cutoff_0.90_subsettedVCFs/*vcf; do
    Rscript genotypePlot_VCFs.R --VCF ${vcf}
done
```

## Data visalization - use genotype plot to visualize vcf files

```bash
# Continue working on local computer to generate scripts and outputs!
# Use genotype_plot package from https://github.com/JimWhiting91/genotype_plot
# genotypePlot_VCFs.R script - uses vcf file (subset by derived sites) such as CM071426.1_subset_cutoff_0.99.vcf and outputs visualization of
# genotypes across chromosome

```

## Data visualization - create an R script to visualize site designation along a chromosome
```bash
# Continue working on local computer to generate scripts and outputs!
# chromHeatMap.R script - uses designation file such as cutoff_0.90_SAHA.tsv or pruned_cutoff_0.90_SAHA.tsv and outputs heatmap showing site des along chromosomes


for file in *0.90_HW_genotypeFreq_Z.tsv; do echo -e "\n==> $file <=="; cat "$file"; done | less -S


```
# Linkage prune again, just for 0.99! See how plots and stats change!
## Generate a list of prune.in sites!
```bash
# Return to linkage pruning directory
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning

# Move to the directory that is storing the PLINK output
cd VCF_subsettedByDes_pruning

# Request interactive
interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load plink/1.90b4.9

# create environmental variables to store paths
VCFs_subsetted="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/InputVCFs/cutoff_0.99_subsettedVCFs"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning"

# just for 0.99 cutoff, this is the cutoff we are going to move forward with!
for vcf in ${VCFs_subsetted}/*vcf ; do # for the VCFs in the 0.99 cutoff dir

    chromID=$(basename "$vcf" .vcf | cut -d "_" -f 1) # pull the chromosome from the vcf file name
    cutoff=$(basename "$vcf" .vcf | cut -d "_" -f 4) # pull the cutoff from the vcf file name


    # Allow extra chromosome - we have a Z chromosome and PLINK was designed for humans
    # Set missing var IDS - so PLINK will use chromosome ID and position as the unique ID (chromID_pos, this format will match nicely with des file)
    # indep pairwise - 50 = window (Kb,) 10 = window step size (bp), 0.1 = r2 value, linkage level we'll accept (prune if > 0.1)
    plink --vcf ${vcf} --allow-extra-chr --set-missing-var-ids @_# --indep-pairwise 50 10 0.1 --out "${output_dir}/${chromID}_cutoff_${cutoff}_pruned_window50" # reducing r2 to 0.09 only reduces the number of sites by 4!

    plink --vcf ${vcf} --allow-extra-chr --set-missing-var-ids @_# --indep-pairwise 200 10 0.1 --out "${output_dir}/${chromID}_cutoff_${cutoff}_pruned_window200" # recall, originall window we set was 450 (267 sites), with 400 window (293 sites)
    
done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning
mkdir Window50_pruned_0.99_cutoff Window200_pruned_0.99_cutoff

mv *cutoff_0.99_pruned_window50* Window50_pruned_0.99_cutoff
mv *cutoff_0.99_pruned_window200* Window200_pruned_0.99_cutoff

# Create concatenated version of pruned in list for the 0.99 cutoffs
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning # if not there already

cat Window50_pruned_0.99_cutoff/*prune.in > prune_in_0.99_cutoff_window50 # list of [chromID]_[positions] to keep for 0.99 cutoff, 2070 sites
cat Window200_pruned_0.99_cutoff/*prune.in > prune_in_0.99_cutoff_window200 # list of [chromID]_[positions] to keep for 0.99 cutoff, 537 sites

```
Outputs: Window200_pruned_0.99_cutoff Window50_pruned_0.99_cutoff directories wich store prune.in, prune.out, .log, .nosex outputs AND the prune_in_0.99_cutoff_window200 prune_in_0.99_cutoff_window50 lists which shows all the [chromID]_[positions] to keep after pruning with the different window sizes

## Subset the designation files based on prune.in sites

```bash

# Subset the 0.99 designation files for prune.in sites
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files # move to the dir that stores the pruned des files

# Request an interactive session
interactive -A naiss2023-5-262 -n 20 -t 2:00:00 

# Directory containing the files, for window size 200 at 0.99 cutoff
designation_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/09_DesignateAlleles/Outputs/Designations_VCFsansMaxMissing" # very long, save into environmental variable!
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files" # also set this to ensure output goes where we want
prune_in_file="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff_window200"

# Iterate over each file in the input directory
for des_dir in "$designation_dir"/cutoff_0.99_sepByChrom_*; do # SAHA and sansSAHA

    for des_file in "$des_dir"/*.tsv; do


        file_basename=$(basename ${des_file} .tsv | cut -d "_" -f 1,3-5 ) # e.g. CM071462.1_cutoff_0.99_sansSAHA


        pruned_output="${output_dir}/${file_basename}_pruned_window200.tsv" # create variables to store output file names, will be shorter to call in further down
        > "$pruned_output" # initiate empty files for output to append to
        
        while IFS= read -r line; do # read designation file line by line

            if [[ "$line" =~ ^# ]]; then # If header line, starts with "#"
            echo "$line" >> "$pruned_output" # keep the header in new output file, but don't scan header for chromID_position
            continue
            fi
            
            locus_col=$(echo "$line" | cut -f 1 | cut -d "_" -f 1-2 ) # pull the locus from column 1, then isolate [chromID]_[position]
            
            if grep -qFx "$locus_col" "$prune_in_file"; then # checks the [chromID]_[position] from the site des file against the list of retained sites post-pruning

            echo "$line" >> "$pruned_output" # if exists, then print designation file line to the output

            fi
        done < "$des_file"

    done

done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files

mkdir pruned_desFiles_sepByChrom_0.99_cutoff_window200
mv *_pruned_window200.tsv pruned_desFiles_sepByChrom_0.99_cutoff_window200


# Repeat for window size 50 at 0.99 cutoff
# Only need to reset this!
prune_in_file="/proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff_window50"

# Iterate over each file in the input directory
for des_dir in "$designation_dir"/cutoff_0.99_sepByChrom_*; do # SAHA and sansSAHA

    for des_file in "$des_dir"/*.tsv; do


        file_basename=$(basename ${des_file} .tsv | cut -d "_" -f 1,3-5 ) # e.g. CM071462.1_cutoff_0.99_sansSAHA


        pruned_output="${output_dir}/${file_basename}_pruned_window50.tsv" # create variables to store output file names, will be shorter to call in further down
        > "$pruned_output" # initiate empty files for output to append to
        
        while IFS= read -r line; do # read designation file line by line

            if [[ "$line" =~ ^# ]]; then # If header line, starts with "#"
            echo "$line" >> "$pruned_output" # keep the header in new output file, but don't scan header for chromID_position
            continue
            fi
            
            locus_col=$(echo "$line" | cut -f 1 | cut -d "_" -f 1-2 ) # pull the locus from column 1, then isolate [chromID]_[position]
            
            if grep -qFx "$locus_col" "$prune_in_file"; then # checks the [chromID]_[position] from the site des file against the list of retained sites post-pruning

            echo "$line" >> "$pruned_output" # if exists, then print designation file line to the output

            fi
        done < "$des_file"

    done

done

# Sort the output files
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files

mkdir pruned_desFiles_sepByChrom_0.99_cutoff_window50
mv *_pruned_window50.tsv pruned_desFiles_sepByChrom_0.99_cutoff_window50


# Create a concatenated version of the TSVs at for each window size, reuse code from above
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/Pruned_designation_files # if not there already

for dir in pruned_desFiles_sepByChrom_0.99_cutoff_window*; do # pulls the window 200 dir and window 50 dir

    window=$(basename "$dir" | cut -d "_" -f 6 ) # pulls the window from the dir name, e.g. window200 

    for SAHA_tsv in ${dir}/*_SAHA_pruned_${window}.tsv; do # matches to the correct tsv

        # Will include each of the headers from the independent designation files
        cat ${SAHA_tsv} >> "./pruned_cutoff_0.99_SAHA_${window}.tsv" # window200 - 578 lines incl headers, window50 - 2111 lines incl headers

    done

    for sansSAHA_tsv in ${dir}/*_sansSAHA_pruned_${window}.tsv; do

        cat ${sansSAHA_tsv} >> "./pruned_cutoff_0.99_sansSAHA_${window}.tsv" # window200 - 578 lines incl headers, window50 - 2111 lines incl headers

    done
done 


# Only keep the first header, remove all the duplicate headers throughout the file
for comp_designation_file in *window*.tsv; do
    awk '/^#/ && !header_seen {print; header_seen=1; next} !/^#/' ${comp_designation_file} > "${comp_designation_file}_temp"

    # overwrite the file to reflect the temp file with just one header
    mv ${comp_designation_file}_temp "${comp_designation_file}" # window200 - 538 lines incl header, window50 - 2071 lines incl header
done
```
Outputs: 4 output files (1 for each window SAHA/sansSAHA combo in the Pruned_designation_files directory). Files are named pruned_cutoff_0.99_[sansSAHA/SAHA]_window[windowSize].tsv. 

## Copy the newly pruned designation files to the local computer and re-run desFiles.R and tests.R

# Visualize sites across chromosomes and see how it changes with different linkage pruning parameters 

## Create vcf.gz files of VCFs at 0.99 cutoff that have been separated by SD and HD and have been pruned at different window sizes
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna
mkdir 14_KaryoploteRprep

# We need pruned designation files in VCF format separated by SD and HD! 

# Recall from previous steps! We already have 0.99 cutoff VCFs separated by SD and HD, located here: /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD!

# Recall from previous steps! We already have a list of prune.in sites for 450, 200, and 50 kb window sizes, located here: /proj/sparrowhybridization/Pitaliae/working/Arianna/12_LinkagePruning/VCF_subsettedByDes_pruning
```
```bash
# Create the VCFs for 0.99 cutoff, sep by SD and HD, pruned

cd /proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep # if not already here

# Use the prune.in sites list we used previously to create a Regions file for bcftools! chrom\tposition
cat ../12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff | tr "_" "\t" > prune_in_0.99_cutoff_window450_bcftoolsFormat # 267 sites
cat ../12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff_window200 | tr "_" "\t" > prune_in_0.99_cutoff_window200_bcftoolsFormat # 537 sites
cat ../12_LinkagePruning/VCF_subsettedByDes_pruning/prune_in_0.99_cutoff_window50 | tr "_" "\t" > prune_in_0.99_cutoff_window50_bcftoolsFormat # 2070 sites

# Create bgzipped versions of the 0.99 cutoff VCFs that have been separated by SD & HD

# bgzip the subsetted VCFs
mkdir Subsetted_sepHDSD_VCFs_bgzipped

interactive -A naiss2023-5-262 -n 20 -t 1:00:00
module load bioinfo-tools
module load bcftools/1.14

vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_sepHDSD_VCFs_bgzipped"

for vcf_dir in "$vcf_parent_dir"/VCFs_subsetted_*D; do

    vcf_dir_name=$(basename "$vcf_dir" ) # pulls just the name (e.g. VCFs_subsetted_HD), not whole path

    for vcf in "$vcf_dir"/*vcf; do
        
        vcf_name=$(basename "$vcf" ) # keep the .vcf ending!

        sub_output_dir="${output_dir}/pruned_${vcf_dir_name}_bgzipped"

        echo ${vcf_name}

        mkdir -p ${sub_output_dir} # will not override existing dir
    
        bcftools view ${vcf} -Oz --threads 20 -o "${sub_output_dir}/${vcf_name}.gz" 

    done

done # took ~2 minutes
```
Outputs: 41 bgzipped vcf files for SD 0.99 (0.99 cut) and 41 for HD (0.99 cut)! Located in Subsetted_sepHDSD_VCFs_bgzipped/VCFs_subsetted_[designation]_bgzipped/[.vcf files]

```bash
# Now index the compressed subsetted vcfs
vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_sepHDSD_VCFs_bgzipped"

for vcf_dir in "$vcf_parent_dir"/VCFs_subsetted_*D_bgzipped; do

    for vcf in "$vcf_dir"/*vcf.gz; do
        
        bcftools index ${vcf}

    done

done # took ~2 minutes

```
Outputs: each of the vcf.gz files that were outputted from above should be accompanied by a .csi file, 82 in total (41 for each cutoff)

```bash

# Now run bcftools to only keep prune.in sites for each window!
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep
mkdir Subsetted_VCFs_linkagePruned_varyWindowSizes

vcf_parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_sepHDSD_VCFs_bgzipped"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_VCFs_linkagePruned_varyWindowSizes"


for vcf_dir in "$vcf_parent_dir"/VCFs_subsetted_*D_bgzipped; do

    vcf_dir_name=$(basename "$vcf_dir" ) # pulls just the name, not whole path
    designation=$(basename "$vcf_dir" | cut -d "_" -f 4 )

    for vcf in "$vcf_dir"/*vcf.gz; do
        
        vcf_name=$(basename "$vcf" .vcf.gz ) # file name without .vcf.gz

        sub_output_dir="${output_dir}/pruned_${vcf_dir_name}"

        mkdir -p ${sub_output_dir} # will not override existing dir

        # use the cutoff associated prune.in list
        bcftools view ${vcf} -R prune_in_0.99_cutoff_window450_bcftoolsFormat -Oz -o "${sub_output_dir}/${vcf_name}_pruned_window450.vcf.gz" 
        bcftools view ${vcf} -R prune_in_0.99_cutoff_window200_bcftoolsFormat -Oz -o "${sub_output_dir}/${vcf_name}_pruned_window200.vcf.gz" 
        bcftools view ${vcf} -R prune_in_0.99_cutoff_window50_bcftoolsFormat -Oz -o "${sub_output_dir}/${vcf_name}_pruned_window50.vcf.gz" 
    

    done

done # took ~3 minutes

# Sort the output into directories based on window size

for vcf_dir in Subsetted_VCFs_linkagePruned_varyWindowSizes/pruned_VCFs_subsetted_*D_bgzipped; do

    designation=$(basename "$vcf_dir" | cut -d "_" -f 4 )

    for vcf in "$vcf_dir"/*vcf.gz; do

        window=$(basename "$vcf" .vcf.gz | cut -d "_" -f 7 ) # will pull window from file name (e.g. window50)

        output_dir="Subsetted_VCFs_linkagePruned_varyWindowSizes/pruned_VCFs_subsetted_${designation}_bgzipped/${designation}_${window}" # in the directory where the VCF files currently are
        #echo ${output_dir}

        mkdir -p ${output_dir}  # will not override existing dir

        mv ${vcf} "${output_dir}"

    done

done

```
Outputs: 246 vcf.gz files! Sorted like this: Subsetted_VCFs_linkagePruned_varyWindowSizes/pruned_VCFs_subsetted_[designation]_bgzippedVCFs/[desiganation]_[windowsize]/*vcf.gz. 41 vcf.gz files per each designation & window combination!

## Concatenate the separate chromsomes into genome VCFs for each des and window! Store in Subsetted_VCFs_linkagePruned_varyWindowSizes
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_VCFs_linkagePruned_varyWindowSizes

interactive -A naiss2023-5-262 -n 20 -t 1:00:00 # if old session done already
module load bioinfo-tools
module load bcftools/1.14

parent_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_VCFs_linkagePruned_varyWindowSizes"

# Create files with lists to paths!
for des_dir in "$parent_dir"/pruned_VCFs_subsetted_*D_bgzipped; do

    for window_dir in "$des_dir"/*D_window*; do

        paths_file_name=$(basename "$window_dir") # e.g. HD_window200

        for vcf in "$window_dir"/*vcf.gz; do

            echo ${vcf} >> "${paths_file_name}_allChroms.list"

        done

    done

done # outputted in Subsetted_VCFs_linkagePruned_varyWindowSizes


# Also create file with list to paths for non-pruned version!
for des_dir in /proj/sparrowhybridization/Pitaliae/working/Arianna/11_GOanalysis/VCFs_subsetted_SD_HD/VCFs_subsetted_*D; do

    paths_file_name=$(basename "$des_dir") # e.g. HD_window200


    for vcf in "$des_dir"/*vcf; do

        echo ${vcf} >> "${paths_file_name}_nonPruned_allChroms.list"

    done

done


# Create the concatenated versions
for list in "$parent_dir"/*window*list; do
    echo ${list}

    file_name=$(basename "$list" .list) #pulls basename!

    bcftools concat -f ${list} -Oz -o  "${file_name}.vcf.gz"

done # outputted in Subsetted_VCFs_linkagePruned_varyWindowSizes

bcftools concat -f VCFs_subsetted_HD_nonPruned_allChroms.list -Oz -o HD_nonPruned_allChroms.vcf
bcftools concat -f VCFs_subsetted_SD_nonPruned_allChroms.list -Oz -o SD_nonPruned_allChroms.vcf


rm *list # clean up by removing path lists

```
Outputs: 6 .vcf.gz output files in Subsetted_VCFs_linkagePruned_varyWindowSizes! name [siteDes]_[windowSize]_allChroms.vcf.gz! 2 .vcf files for the non-pruned vcfs separated by HD and SD. 


## Run bedopps to convert these vcfs into bed format
```bash
cd /proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep
mkdir BedFormat_liknagePruned_varyWindowSize

interactive -A naiss2023-5-262 -n 20 -t 2:00:00 # new interactive session if necessary
module load bioinfo-tools
module load BEDOPS/2.4.39

vcf_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/Subsetted_VCFs_linkagePruned_varyWindowSizes"
output_dir="/proj/sparrowhybridization/Pitaliae/working/Arianna/14_KaryoploteRprep/BedFormat_liknagePruned_varyWindowSize"


for vcf in "$vcf_dir"/*vcf.gz; do

    vcf_name=$(basename "$vcf" .vcf.gz) #pulls basename!

    zcat ${vcf} | vcf2bed > "${output_dir}/${vcf_name}.bed" # use zcat rather than bcftools, don't have to load another package

done

# also run on the two .vcf files (uncompressed)
cat ../Subsetted_VCFs_linkagePruned_varyWindowSizes/HD_nonPruned_allChroms.vcf | vcf2bed > HD_nonPruned_allChroms.bed
cat ../Subsetted_VCFs_linkagePruned_varyWindowSizes/SD_nonPruned_allChroms.vcf | vcf2bed > SD_nonPruned_allChroms.bed
```
Output: 8 .bed files! 1 for each designation (SD and HD) and pruning level (no pruning 50kb window, 200 kb window, 450 kb window)

## Move to desktop and use karyoploteR (v.1.28.0)  in R to visualize!